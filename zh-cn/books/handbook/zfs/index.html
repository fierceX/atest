<!DOCTYPE html>
<html class="theme-light" lang="zh-cn">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="ZFS is an advanced file system designed to solve major problems found in previous storage subsystem software"/>
  <meta name="keywords" content="91, 34, 90, 70, 83, 34, 44, 32, 34, 102, 105, 108, 101, 115, 121, 115, 116, 101, 109, 34, 44, 32, 34, 97, 100, 109, 105, 110, 105, 115, 116, 114, 97, 116, 105, 111, 110, 34, 44, 32, 34, 122, 112, 111, 111, 108, 34, 44, 32, 34, 102, 101, 97, 116, 117, 114, 101, 115, 34, 44, 32, 34, 116, 101, 114, 109, 105, 110, 111, 108, 111, 103, 121, 34, 44, 32, 34, 82, 65, 73, 68, 45, 90, 34, 93"/>
  <meta name="copyright" content="1995-2023 The FreeBSD Foundation" />
  <link rel="canonical" href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/" />

  <title>Chapter 22. The Z File System (ZFS) |  FreeBSD Documentation Portal</title>

  <meta name="theme-color" content="#790000">
  <meta name="color-scheme" content="system light dark high-contrast">

    <link rel="shortcut icon" href="https://free.bsd-doc.org/favicon.ico">
    <link rel="stylesheet" href="/styles/main.min.css">
    <link rel="stylesheet" href="https://free.bsd-doc.org/css/font-awesome-min.css">
    <script defer src="/js/theme-chooser.min.js"></script>
    <script defer src="/js/copy-clipboard.min.js"></script>
    <script defer src="/js/search.min.js"></script>

  
  
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:domain" content="docs.FreeBSD.org"/>
    <meta name="twitter:site" content="@freebsd"/>
    <meta name="twitter:url" content="https://twitter.com/freebsd"/>
    <meta property="og:title" content="Chapter 22. The Z File System (ZFS)" />
    <meta property="og:description" content="ZFS is an advanced file system designed to solve major problems found in previous storage subsystem software" />
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://free.bsd-doc.orgfavicon.ico"/>
    <meta property="og:image:alt" content="FreeBSD Logo">
    <meta property="og:locale" content="zh-cn" />
    <meta property="og:url" content="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/" />
    <meta property="og:site_name" content="FreeBSD Documentation Portal" />
    <script type="application/ld+json">
      {
        "@context": "http://schema.org",
        "@type": "Article",
        "url": "https:\/\/free.bsd-doc.org\/zh-cn\/books\/handbook\/zfs\/",
        "name": "FreeBSD Documentation Portal",
        "headline": "FreeBSD Documentation Portal",
        "description": "FreeBSD Documentation Portal"
      }
    </script>
    

  
</head>


  <body>
    <header>
  <div class="header-container">
    <div class="logo-menu-bars-container">
      <a href="https://free.bsd-doc.org" class="logo">
        <img src="https://free.bsd-doc.org/images/FreeBSD-monochromatic.svg" width="160" height="50" alt="FreeBSD logo" />
      </a>
      <label class="menu-bars" for="menu-bars">
        <i class="fa fa-bars" aria-hidden="true"></i>
      </label>
    </div>
    <input id="menu-bars" type="checkbox" />
    
    <div class="search-donate-container">
      
      <div class="donate">
        <a href="https://github.com/fierceX/freebsd-doc-cn" target="_blank">
          <span class="heart">♥</span>
          GitHub
        </a>
      </div>
    </div>
  </div>
</header>

    
<input type="checkbox" class="hidden toggle" id="menu-control">
<main class="main-wrapper-book">
  <a id="top"></a>
  
  <aside class="book-menu">
    <div class="book-menu-content">
      <input id="search-book" type="text" placeholder="Search" aria-label="Search" maxlength="128" />
      <nav id="MenuContents">
        
  <ul>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-8ada319c45e780947f82b569049480cc" class="toggle"  />
          <label  class="icon cursor"  for="chapter-8ada319c45e780947f82b569049480cc"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/">
              Preface
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-audience">Intended Audience</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-changes-from4">Fourth Edition</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-changes-from3">Third Edition</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-changes-from2">Second Edition (2004)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-changes">First Edition (2001)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-overview">Organization of This Book</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-conv">Conventions used in this book</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/preface/#preface-acknowledgements">Acknowledgments</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-32a83d18a326852a8d9ae2582360bbc8" class="toggle"  />
          <label  for="chapter-32a83d18a326852a8d9ae2582360bbc8"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/parti/">
              第一部分：入门指南
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-a4909db05d872d16d5be86a57cbc2dc7" class="toggle"  />
          <label  class="icon cursor"  for="chapter-a4909db05d872d16d5be86a57cbc2dc7"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/introduction/">
              第一章 引言
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/introduction/#introduction-synopsis">1.1. 简介</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/introduction/#nutshell">1.2. 欢迎来到 FreeBSD ！</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/introduction/#history">1.3. 关于 FreeBSD 项目</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-3c2455cadf89eadd3f6e77461085d5e9" class="toggle"  />
          <label  class="icon cursor"  for="chapter-3c2455cadf89eadd3f6e77461085d5e9"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/">
              第二章 安装 FreeBSD
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-synopsis">2.1. 简介</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-hardware">2.2. 最低硬件要求</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-pre">2.3. 安装前的任务</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-start">2.4. 开始安装</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#using-bsdinstall">2.5. 使用 bsdinstall</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-partitioning">2.6. 分配磁盘空间</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-fetching-distribution">2.7. 获取分发文件</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-post">2.8. 网络接口，账户，时区，服务和加固</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#bsdinstall-install-trouble">2.9. 故障排除</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bsdinstall/#using-live-cd">2.10. 使用 Live CD</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-709481b75d22ad36960f7ac166b3306c" class="toggle"  />
          <label  class="icon cursor"  for="chapter-709481b75d22ad36960f7ac166b3306c"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/">
              第三章. FreeBSD 基础知识
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#basics-synopsis">3.1. 概要</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#consoles">3.2. 虚拟控制台和终端</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#users-synopsis">3.3. 用户和基本账户管理</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#permissions">3.4. 权限</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#dirstructure">3.5. 目录结构</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#disk-organization">3.6. 磁盘组织</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#mount-unmount">3.7. 挂载和卸载文件系统</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#basics-processes">3.8. 进程和守护进程</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#shells">3.9. Shells</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#editors">3.10. 文本编辑器</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#basics-devices">3.11. 设备和设备节点</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/basics/#basics-more-information">3.12. 手册页</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-58c050d31a1fbecd859248dacefeb567" class="toggle"  />
          <label  class="icon cursor"  for="chapter-58c050d31a1fbecd859248dacefeb567"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/">
              第四章 安装应用程序：软件包和 Ports
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-synopsis">4.1. 简介</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-overview">4.2. 软件安装概述</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-finding-applications">4.3. 寻找软件</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#pkgng-intro">4.4. 使用 pkg 进行二进制包管理</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-using">4.5. 使用 Ports 集合</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-poudriere">4.6. 使用 poudriere 构建软件包</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-nextsteps">4.7. 安装后的考虑事项</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ports/#ports-broken">4.8. 处理损坏的 ports</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-e5a000ce7f50680a180e2c1b83c121bc" class="toggle"  />
          <label  class="icon cursor"  for="chapter-e5a000ce7f50680a180e2c1b83c121bc"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/">
              Chapter 5. The X Window System
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/#x11-synopsis">5.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/#x-install">5.2. Installing Xorg</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/#x-graphic-card-drivers">5.3. Graphic card drivers</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/#x-config">5.4. Xorg Configuration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/x11/#x-fonts">5.5. Using Fonts in Xorg</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-235af08fb87fd7a58003348706d2111d" class="toggle"  />
          <label  class="icon cursor"  for="chapter-235af08fb87fd7a58003348706d2111d"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/">
              Chapter 6. Wayland
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-synopsis">6.1. Wayland Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-wayfire">6.2. The Wayfire Compositor</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-hikari">6.3. The Hikari Compositor</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-sway">6.4. The Sway Compositor</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-xwayland">6.5. Using Xwayland</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-remotedesktop">6.6. Remote Desktop Using VNC</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-ly">6.7. Wayland Login Manager</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wayland/#wayland-utilities">6.8. Useful Utilities</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-4de18483dde20bf9ed0024176afd09da" class="toggle"  />
          <label  class="icon cursor"  for="chapter-4de18483dde20bf9ed0024176afd09da"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/">
              Chapter 7. Network
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#network-synopsis">7.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#config-network-setup">7.2. Setting up the Network</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#config-network-connection">7.3. Wired Networks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#network-wireless">7.4. Wireless Networks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#hostname">7.5. Hostname</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#dns">7.6. DNS</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network/#troubleshooting">7.7. Troubleshooting</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-5eb30759c3a2873e716dc31d7760054c" class="toggle"  />
          <label  for="chapter-5eb30759c3a2873e716dc31d7760054c"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/partii/">
              第二部分：常见任务
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-c96504f017c56f4bc7f71548cccf5382" class="toggle"  />
          <label  class="icon cursor"  for="chapter-c96504f017c56f4bc7f71548cccf5382"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/">
              Chapter 8. Desktop Environments
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-synopsis">8.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-environments">8.2. Desktop Environments</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-browsers">8.3. Browsers</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-development">8.4. Development tools</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-productivity">8.5. Desktop office productivity</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-viewers">8.6. Document Viewers</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/desktop/#desktop-finance">8.7. Finance</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-e1898af36aaacc168e13b673d25b908f" class="toggle"  />
          <label  class="icon cursor"  for="chapter-e1898af36aaacc168e13b673d25b908f"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/">
              Chapter 9. Multimedia
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#multimedia-synopsis">9.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#sound-setup">9.2. Setting Up the Sound Card</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#audio-ports">9.3. Audio players</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#video-ports">9.4. Video players</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#conferencing-meetings">9.5. Conferencing and Meetings</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/multimedia/#scanners">9.6. Image Scanners</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-6926664836e63c5cae4b2fa60855f91d" class="toggle"  />
          <label  class="icon cursor"  for="chapter-6926664836e63c5cae4b2fa60855f91d"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/">
              Chapter 10. Configuring the FreeBSD Kernel
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-synopsis">10.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-custom-kernel">10.2. Why Build a Custom Kernel?</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-devices">10.3. Finding the System Hardware</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-config">10.4. The Configuration File</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-building">10.5. Building and Installing a Custom Kernel</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/kernelconfig/#kernelconfig-trouble">10.6. If Something Goes Wrong</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-19d36bd70a320268f0ea1f66e4f3e224" class="toggle"  />
          <label  class="icon cursor"  for="chapter-19d36bd70a320268f0ea1f66e4f3e224"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/">
              Chapter 11. Printing
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-quick-start">11.1. Quick Start</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-connections">11.2. Printer Connections</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-pdls">11.3. Common Page Description Languages</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-direct">11.4. Direct Printing</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-lpd">11.5. LPD (Line Printer Daemon)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/printing/#printing-other">11.6. Other Printing Systems</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-a07f539e6c18f2db4da5dbb2b5bb1e79" class="toggle"  />
          <label  class="icon cursor"  for="chapter-a07f539e6c18f2db4da5dbb2b5bb1e79"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/linuxemu/">
              Chapter 12. Linux Binary Compatibility
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/linuxemu/#linuxemu-synopsis">12.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/linuxemu/#linuxemu-lbc-install">12.2. Configuring Linux Binary Compatibility</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/linuxemu/#linux-userlands">12.3. Linux userlands</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/linuxemu/#linuxemu-advanced">12.4. Advanced Topics</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-af234c193ce95c480af0ff5ec3f32f5c" class="toggle"  />
          <label  class="icon cursor"  for="chapter-af234c193ce95c480af0ff5ec3f32f5c"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/">
              Chapter 13. WINE
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#wine-synopsis">13.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#wine-overview-concepts">13.2. WINE Overview & Concepts</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#installing-wine-on-freebsd">13.3. Installing WINE on FreeBSD</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#running-first-wine-program">13.4. Running a First WINE Program on FreeBSD</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#configuring-wine-installation">13.5. Configuring WINE Installation</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#wine-management-guis">13.6. WINE Management GUIs</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#wine-in-multi-user-os-installations">13.7. WINE in Multi-User FreeBSD Installations</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/wine/#wine-on-os-faq">13.8. WINE on FreeBSD FAQ</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-51416fdea9d5a1dfa1c2718cd825213a" class="toggle"  />
          <label  for="chapter-51416fdea9d5a1dfa1c2718cd825213a"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/partiii/">
              第三部分：系统管理
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-532d0380ba66d66e033765a0b6930ee7" class="toggle"  />
          <label  class="icon cursor"  for="chapter-532d0380ba66d66e033765a0b6930ee7"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/">
              Chapter 14. Configuration, Services, Logging and Power Management
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#config-synopsis">14.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#configtuning-configfiles">14.2. Configuration Files</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#configtuning-rcd">14.3. Managing Services in FreeBSD</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#cron-periodic">14.4. Cron and Periodic</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#configtuning-syslog">14.5. Configuring System Logging</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#acpi-overview">14.6. Power and Resource Management</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/config/#adding-swap-space">14.7. Adding Swap Space</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-d05f41680fb7e3d68b354ae03de368b1" class="toggle"  />
          <label  class="icon cursor"  for="chapter-d05f41680fb7e3d68b354ae03de368b1"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/boot/">
              Chapter 15. The FreeBSD Booting Process
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/boot/#boot-synopsis">15.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/boot/#boot-introduction">15.2. FreeBSD Boot Process</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/boot/#device-hints">15.3. Device Hints</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/boot/#boot-shutdown">15.4. Shutdown Sequence</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-513e8e407c57354df334059688e9b60a" class="toggle"  />
          <label  class="icon cursor"  for="chapter-513e8e407c57354df334059688e9b60a"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/">
              Chapter 16. Security
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-synopsis">16.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-intro">16.2. Introduction</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#sec-accounts">16.3. Securing Accounts</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-ids">16.4. Intrusion Detection System (IDS)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-secure-levels">16.5. Secure levels</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-file-flags">16.6. File flags</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#openssh">16.7. OpenSSH</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#openssl">16.8. OpenSSL</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#kerberos5">16.9. Kerberos</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#tcpwrappers">16.10. TCP Wrappers</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#fs-acl">16.11. Access Control Lists</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#capsicum">16.12. Capsicum</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-accounting">16.13. Process Accounting</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-resourcelimits">16.14. Resource Limits</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-pkg">16.15. Monitoring Third Party Security Issues</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/security/#security-advisories">16.16. FreeBSD Security Advisories</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-0f7d2858504c83a029928c048d394e2a" class="toggle"  />
          <label  class="icon cursor"  for="chapter-0f7d2858504c83a029928c048d394e2a"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/">
              Chapter 17. Jails and Containers
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jails-synopsis">17.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jail-types">17.2. Jail Types</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#host-configuration">17.3. Host Configuration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#classic-jail">17.4. Classic Jail (Thick Jail)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#thin-jail">17.5. Thin Jails</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jail-management">17.6. Jail Management</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jail-upgrading">17.7. Jail Upgrading</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jail-resource-limits">17.8. Jail Resource Limits</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/jails/#jail-managers-and-containers">17.9. Jail Managers and Containers</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-15be125ce5997f51c5499e3a0dee6c2c" class="toggle"  />
          <label  class="icon cursor"  for="chapter-15be125ce5997f51c5499e3a0dee6c2c"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/">
              Chapter 18. Mandatory Access Control
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-synopsis">18.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-inline-glossary">18.2. Key Terms</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-understandlabel">18.3. Understanding MAC Labels</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-planning">18.4. Planning the Security Configuration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-policies">18.5. Available MAC Policies</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-userlocked">18.6. User Lock Down</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-implementing">18.7. Nagios in a MAC Jail</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mac/#mac-troubleshoot">18.8. Troubleshooting the MAC Framework</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-3f7e8d763b17c7d0c44c60505cf0feea" class="toggle"  />
          <label  class="icon cursor"  for="chapter-3f7e8d763b17c7d0c44c60505cf0feea"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/audit/">
              Chapter 19. Security Event Auditing
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/audit/#audit-synopsis">19.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/audit/#audit-inline-glossary">19.2. Key Terms</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/audit/#audit-config">19.3. Audit Configuration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/audit/#audit-administration">19.4. Working with Audit Trails</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-7bb1cb2287cd08a3672cc878e604f0d0" class="toggle"  />
          <label  class="icon cursor"  for="chapter-7bb1cb2287cd08a3672cc878e604f0d0"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/">
              Chapter 20. Storage
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-synopsis">20.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-adding">20.2. Adding Disks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-growing">20.3. Resizing and Growing Disks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#usb-disks">20.4. USB Storage Devices</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#creating-cds">20.5. Creating and Using CD Media</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#creating-dvds">20.6. Creating and Using DVD Media</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#floppies">20.7. Creating and Using Floppy Disks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#using-ntfs">20.8. Using NTFS Disks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#backup-basics">20.9. Backup Basics</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-virtual">20.10. Memory Disks</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#snapshots">20.11. File System Snapshots</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#quotas">20.12. Disk Quotas</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-encrypting">20.13. Encrypting Disk Partitions</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#swap-encrypting">20.14. Encrypting Swap</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/disks/#disks-hast">20.15. Highly Available Storage (HAST)</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-dec5fc4f0100e5030db0fb188d8d4b2f" class="toggle"  />
          <label  class="icon cursor"  for="chapter-dec5fc4f0100e5030db0fb188d8d4b2f"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/">
              Chapter 21. GEOM: Modular Disk Transformation Framework
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-synopsis">21.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-striping">21.2. RAID0 - Striping</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-mirror">21.3. RAID1 - Mirroring</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-raid3">21.4. RAID3 - Byte-level Striping with Dedicated Parity</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-graid">21.5. Software RAID Devices</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-ggate">21.6. GEOM Gate Network</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-glabel">21.7. Labeling Disk Devices</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/geom/#geom-gjournal">21.8. UFS Journaling Through GEOM</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-7f1657a05627dd16bad31d4c7f675604" class="toggle"  checked  />
          <label  class="icon cursor"  for="chapter-7f1657a05627dd16bad31d4c7f675604"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/">
              Chapter 22. The Z File System (ZFS)
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-differences">22.1. What Makes ZFS Different</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-quickstart">22.2. Quick Start Guide</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-zpool">22.3. <code>zpool</code> Administration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-zfs">22.4. <code>zfs</code> Administration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-zfs-allow">22.5. Delegated Administration</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-advanced">22.6. Advanced Topics</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-links">22.7. Further Resources</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/zfs/#zfs-term">22.8. ZFS Features and Terminology</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-4c6bf6828e601b79edb2813329ac6604" class="toggle"  />
          <label  class="icon cursor"  for="chapter-4c6bf6828e601b79edb2813329ac6604"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/filesystems/">
              Chapter 23. Other File Systems
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/filesystems/#filesystems-synopsis">23.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/filesystems/#filesystems-linux">23.2. Linux® File Systems</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-ae23289d4b6415f3f878893f9d3ef9b8" class="toggle"  />
          <label  class="icon cursor"  for="chapter-ae23289d4b6415f3f878893f9d3ef9b8"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/">
              Chapter 24. Virtualization
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-synopsis">24.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-guest-parallelsdesktop">24.2. FreeBSD as a Guest on Parallels Desktop for macOS®</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-guest-vmware">24.3. FreeBSD as a Guest on VMware Fusion for macOS®</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-guest-virtualbox">24.4. FreeBSD as a Guest on VirtualBox™</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-host-virtualbox">24.5. FreeBSD as a Host with VirtualBox™</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-host-bhyve">24.6. FreeBSD as a Host with bhyve</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/virtualization/#virtualization-host-xen">24.7. FreeBSD as a Xen™-Host</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-b279d2b738963a45a059268d2fae00d5" class="toggle"  />
          <label  class="icon cursor"  for="chapter-b279d2b738963a45a059268d2fae00d5"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/l10n/">
              Chapter 25. Localization - i18n/L10n Usage and Setup
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/l10n/#l10n-synopsis">25.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/l10n/#using-localization">25.2. Using Localization</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/l10n/#l10n-compiling">25.3. Finding i18n Applications</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/l10n/#lang-setup">25.4. Locale Configuration for Specific Languages</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-9e3f99bd76c861ab2e9a5f31ac421842" class="toggle"  />
          <label  class="icon cursor"  for="chapter-9e3f99bd76c861ab2e9a5f31ac421842"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/">
              Chapter 26. Updating and Upgrading FreeBSD
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#updating-upgrading-synopsis">26.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#updating-upgrading-freebsdupdate">26.2. FreeBSD Update</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#updating-bootcode">26.3. Updating Bootcode</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#updating-upgrading-documentation">26.4. Updating the Documentation Set</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#current-stable">26.5. Tracking a Development Branch</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#makeworld">26.6. Updating FreeBSD from Source</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#small-lan">26.7. Tracking for Multiple Machines</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/cutting-edge/#building-on-non-freebsd-hosts">26.8. Building on non-FreeBSD Hosts</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-3ba999091575eb04ae88235cd0fb1afa" class="toggle"  />
          <label  class="icon cursor"  for="chapter-3ba999091575eb04ae88235cd0fb1afa"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/dtrace/">
              Chapter 27. DTrace
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/dtrace/#dtrace-synopsis">27.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/dtrace/#dtrace-implementation">27.2. Implementation Differences</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/dtrace/#dtrace-enable">27.3. Enabling DTrace Support</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/dtrace/#dtrace-using">27.4. Using DTrace</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-bdb217202a0a3eef4571dd1fcd50863c" class="toggle"  />
          <label  class="icon cursor"  for="chapter-bdb217202a0a3eef4571dd1fcd50863c"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/usb-device-mode/">
              Chapter 28. USB Device Mode / USB OTG
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/usb-device-mode/#usb-device-mode-synopsis">28.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/usb-device-mode/#usb-device-mode-terminals">28.2. USB Virtual Serial Ports</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/usb-device-mode/#usb-device-mode-network">28.3. USB Device Mode Network Interfaces</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/usb-device-mode/#usb-device-mode-storage">28.4. USB Virtual Storage Device</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-6d4439926bb106ac084ecc88cfd3cfa4" class="toggle"  />
          <label  for="chapter-6d4439926bb106ac084ecc88cfd3cfa4"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/partiv/">
              第四部分：网络通信
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-05a353fcb61d8e035f7c43e14725766e" class="toggle"  />
          <label  class="icon cursor"  for="chapter-05a353fcb61d8e035f7c43e14725766e"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/">
              Chapter 29. Serial Communications
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#serial-synopsis">29.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#serial">29.2. Serial Terminology and Hardware</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#term">29.3. Terminals</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#dialup">29.4. Dial-in Service</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#dialout">29.5. Dial-out Service</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/serialcomms/#serialconsole-setup">29.6. Setting Up the Serial Console</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-7f0cbcbf6701b86eba3bbd6fc829f1b8" class="toggle"  />
          <label  class="icon cursor"  for="chapter-7f0cbcbf6701b86eba3bbd6fc829f1b8"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/">
              Chapter 30. PPP
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/#ppp-and-slip-synopsis">30.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/#userppp">30.2. Configuring PPP</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/#ppp-troubleshoot">30.3. Troubleshooting PPP Connections</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/#pppoe">30.4. Using PPP over Ethernet (PPPoE)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/ppp-and-slip/#pppoa">30.5. Using PPP over ATM (PPPoA)</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-9d023b545804f197d195dd9412749943" class="toggle"  />
          <label  class="icon cursor"  for="chapter-9d023b545804f197d195dd9412749943"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/">
              Chapter 31. Electronic Mail
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#mail-synopsis">31.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#mail-using">31.2. Mail Components</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#dragonFly-mail-agent">31.3. DragonFly Mail Agent (DMA)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#sendmail">31.4. Sendmail</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#mail-changingmta">31.5. Changing the Mail Transfer Agent</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#mail-agents">31.6. Mail User Agents</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mail/#mail-advanced">31.7. Advanced Topics</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-42b19e05d30c4135c74b9878818747a9" class="toggle"  />
          <label  class="icon cursor"  for="chapter-42b19e05d30c4135c74b9878818747a9"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/">
              Chapter 32. Network Servers
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-servers-synopsis">32.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-inetd">32.2. The inetd Super-Server</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-nfs">32.3. Network File System (NFS)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-nis">32.4. Network Information System (NIS)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-ldap">32.5. Lightweight Directory Access Protocol (LDAP)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-dhcp">32.6. Dynamic Host Configuration Protocol (DHCP)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-dns">32.7. Domain Name System (DNS)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-apache">32.8. Apache HTTP Server</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-ftp">32.9. File Transfer Protocol (FTP)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-samba">32.10. File and Print Services for Microsoft® Windows® Clients (Samba)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-ntp">32.11. Clock Synchronization with NTP</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/network-servers/#network-iscsi">32.12. iSCSI Initiator and Target Configuration</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-9554a5177d00d0e87ada408809a1fc67" class="toggle"  />
          <label  class="icon cursor"  for="chapter-9554a5177d00d0e87ada408809a1fc67"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/">
              Chapter 33. Firewalls
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-intro">33.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-concepts">33.2. Firewall Concepts</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-pf">33.3. PF</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-ipfw">33.4. IPFW</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-ipf">33.5. IPFILTER (IPF)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/firewalls/#firewalls-blacklistd">33.6. Blacklistd</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-9903a1d844ed803a436595e93ec33572" class="toggle"  />
          <label  class="icon cursor"  for="chapter-9903a1d844ed803a436595e93ec33572"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/">
              Chapter 34. Advanced Networking
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#advanced-networking-synopsis">34.1. Synopsis</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-routing">34.2. Gateways and Routes</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#configtuning-virtual-hosts">34.3. Virtual Hosts</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-advanced-wireless">34.4. Wireless Advanced Authentication</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#wireless-ad-hoc-mode">34.5. Wireless Ad-hoc Mode</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-usb-tethering">34.6. USB Tethering</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-bluetooth">34.7. Bluetooth</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-bridging">34.8. Bridging</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-aggregation">34.9. Link Aggregation and Failover</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-diskless">34.10. Diskless Operation with PXE</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#carp">34.11. Common Address Redundancy Protocol (CARP)</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/advanced-networking/#network-vlan">34.12. VLANs</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-8ca29f99c249993ee9234cb4632af7a5" class="toggle"  />
          <label  for="chapter-8ca29f99c249993ee9234cb4632af7a5"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/partv/">
              第五部分. 附录
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-0229d553ebf23ffeb945be6f32836221" class="toggle"  />
          <label  class="icon cursor"  for="chapter-0229d553ebf23ffeb945be6f32836221"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/mirrors/">
              Appendix A. Obtaining FreeBSD
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mirrors/#mirrors">A.1. Mirrors</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mirrors/#git">A.2. Using Git</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mirrors/#svn">A.3. Using Subversion</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/mirrors/#mirrors-cdrom">A.4. CD and DVD Sets</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-743c99fa278b8138914ff018e40c9231" class="toggle"  />
          <label  class="icon cursor"  for="chapter-743c99fa278b8138914ff018e40c9231"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/bibliography/">
              Appendix B. Bibliography
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bibliography/#bibliography-freebsd">B.1. FreeBSD Bibliography</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bibliography/#bibliography-security">B.2. Security Reference</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bibliography/#bibliography-history">B.3. UNIX® History</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/bibliography/#bibliography-journals">B.4. Periodicals, Journals, and Magazines</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-de7eade4b8b64f298c0cbac44c2278da" class="toggle"  />
          <label  class="icon cursor"  for="chapter-de7eade4b8b64f298c0cbac44c2278da"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/eresources/">
              Appendix C. Resources on the Internet
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/eresources/#eresources-www">C.1. Websites</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/eresources/#eresources-mail">C.2. Mailing Lists</a></li>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/eresources/#eresources-news">C.3. Usenet Newsgroups</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-bfb0d4e71c3c1a18a8b75e03f4b48e26" class="toggle"  />
          <label  class="icon cursor"  for="chapter-bfb0d4e71c3c1a18a8b75e03f4b48e26"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/pgpkeys/">
              Appendix D. OpenPGP Keys
            </a>
            
            
  <ul>
    <li><a href="https://free.bsd-doc.org/zh-cn/books/handbook/pgpkeys/#pgpkeys-officers">D.1. Officers</a></li>
  </ul>

          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-2a8deedf14429128049622ec007c9d3f" class="toggle"  />
          <label  for="chapter-2a8deedf14429128049622ec007c9d3f"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/glossary/">
              FreeBSD Glossary
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-110dbb242491bf200e51e76db171f670" class="toggle"  />
          <label  for="chapter-110dbb242491bf200e51e76db171f670"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/colophon/">
              Colophon
            </a>
            
            
          

        
      </li>
    
      <li>
        

          
          
          

          <input type="checkbox" id="chapter-caffbcba74982e4b26b9263ac0232bc3" class="toggle"  />
          <label  for="chapter-caffbcba74982e4b26b9263ac0232bc3"><a role="button"></a></label>

          
            
            <a href="https://free.bsd-doc.org/zh-cn/books/handbook/introduction/">
              
            </a>
            
            
          

        
      </li>
    
      <li>
        
      </li>
    
  </ul>


      </nav>
    </div>
  </aside>
  
  <div class="book">
    
    <div class="book-menu-mobile">
      <label for="menu-control">
        <span class="menu-control-button">
          <i class="fa fa-list" aria-hidden="true" title="Book menu"></i>
          Book menu
        </span>
      </label>
    </div>
    
    <h1 class="title">Chapter 22. The Z File System (ZFS)</h1>
    
    
      <div class="admonitionblock note">
        <p>
          <i class="fa fa-exclamation-circle" aria-hidden="true"></i>
          如果发现翻译错误，请直接 <a href="https://github.com/fierceX/freebsd-doc-cn/pulls" target="_blank">发起PR修改</a>。
        </p>
      </div>
    
    
    
    <div class="toc-mobile">
      <h3>Table of Contents</h3>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#zfs-differences">22.1. What Makes ZFS Different</a></li>
    <li><a href="#zfs-quickstart">22.2. Quick Start Guide</a></li>
    <li><a href="#zfs-zpool">22.3. <code>zpool</code> Administration</a></li>
    <li><a href="#zfs-zfs">22.4. <code>zfs</code> Administration</a></li>
    <li><a href="#zfs-zfs-allow">22.5. Delegated Administration</a></li>
    <li><a href="#zfs-advanced">22.6. Advanced Topics</a></li>
    <li><a href="#zfs-links">22.7. Further Resources</a></li>
    <li><a href="#zfs-term">22.8. ZFS Features and Terminology</a></li>
  </ul>
</nav>
    </div>
    
      
      
    
    <div class="book-content">
      <div id="preamble">
<div class="sectionbody">

<div class="paragraph">
<p>ZFS is an advanced file system designed to solve major problems found in previous storage subsystem software.</p>
</div>
<div class="paragraph">
<p>Originally developed at Sun™, ongoing open source ZFS development has moved to the <a href="http://open-zfs.org">OpenZFS Project</a>.</p>
</div>
<div class="paragraph">
<p>ZFS has three major design goals:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Data integrity: All data includes a <a href="#zfs-term-checksum">checksum</a> of the data. ZFS calculates checksums and writes them along with the data. When reading that data later, ZFS recalculates the checksums. If the checksums do not match, meaning detecting one or more data errors, ZFS will attempt to automatically correct errors when ditto-, mirror-, or parity-blocks are available.</p>
</li>
<li>
<p>Pooled storage: adding physical storage devices to a pool, and allocating storage space from that shared pool. Space is available to all file systems and volumes, and increases by adding new storage devices to the pool.</p>
</li>
<li>
<p>Performance: caching mechanisms provide increased performance. <a href="#zfs-term-arc">ARC</a> is an advanced memory-based read cache. ZFS provides a second level disk-based read cache with <a href="#zfs-term-l2arc">L2ARC</a>, and a disk-based synchronous write cache named <a href="#zfs-term-zil">ZIL</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A complete list of features and terminology is in <a href="#zfs-term">ZFS Features and Terminology</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-differences">22.1. What Makes ZFS Different<a class="anchor" href="#zfs-differences"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>More than a file system, ZFS is fundamentally different from traditional file systems. Combining the traditionally separate roles of volume manager and file system provides ZFS with unique advantages. The file system is now aware of the underlying structure of the disks. Traditional file systems could exist on a single disk alone at a time. If there were two disks then creating two separate file systems was necessary. A traditional hardware RAID configuration avoided this problem by presenting the operating system with a single logical disk made up of the space provided by physical disks on top of which the operating system placed a file system. Even with software RAID solutions like those provided by GEOM, the UFS file system living on top of the RAID believes it’s dealing with a single device. ZFS&#39; combination of the volume manager and the file system solves this and allows the creation of file systems that all share a pool of available storage. One big advantage of ZFS&#39; awareness of the physical disk layout is that existing file systems grow automatically when adding extra disks to the pool. This new space then becomes available to the file systems. ZFS can also apply different properties to each file system. This makes it useful to create separate file systems and datasets instead of a single monolithic file system.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-quickstart">22.2. Quick Start Guide<a class="anchor" href="#zfs-quickstart"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>FreeBSD can mount ZFS pools and datasets during system initialization. To enable it, add this line to <span class="filename">/etc/rc.conf</span>:</p>
</div>
<div class="literalblock programlisting">
<div class="content">
<pre>zfs_enable=&#34;YES&#34;</pre>
</div>
</div>
<div class="paragraph">
<p>Then start the service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># service zfs start</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The examples in this section assume three SCSI disks with the device names <span class="filename">da0</span>, <span class="filename">da1</span>, and <span class="filename">da2</span>. Users of SATA hardware should instead use <span class="filename">ada</span> device names.</p>
</div>
<div class="sect2">
<h3 id="zfs-quickstart-single-disk-pool">22.2.1. Single Disk Pool<a class="anchor" href="#zfs-quickstart-single-disk-pool"></a></h3>
<div class="paragraph">
<p>To create a simple, non-redundant pool using a single disk device:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create example /dev/da0</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To view the new pool, review the output of <code>df</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</code></pre>
</div>
</div>
<div class="paragraph">
<p>This output shows creating and mounting of the <code>example</code> pool, and that is now accessible as a file system. Create files for users to browse:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cd /example</span>
<span class="c"># ls</span>
<span class="c"># touch testfile</span>
<span class="c"># ls -al</span>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</code></pre>
</div>
</div>
<div class="paragraph">
<p>This pool is not using any advanced ZFS features and properties yet. To create a dataset on this pool with compression enabled:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs create example/compressed</span>
<span class="c"># zfs set compression=gzip example/compressed</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>example/compressed</code> dataset is now a ZFS compressed file system. Try copying some large files to <span class="filename">/example/compressed</span>.</p>
</div>
<div class="paragraph">
<p>Disable compression with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set compression=off example/compressed</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To unmount a file system, use <code>zfs umount</code> and then verify with <code>df</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs umount example/compressed</span>
<span class="c"># df</span>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</code></pre>
</div>
</div>
<div class="paragraph">
<p>To re-mount the file system to make it accessible again, use <code>zfs mount</code> and verify with <code>df</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs mount example/compressed</span>
<span class="c"># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</code></pre>
</div>
</div>
<div class="paragraph">
<p>Running <code>mount</code> shows the pool and file systems:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># mount</span>
/dev/ad0s1a on / <span class="o">(</span>ufs, <span class="nb">local</span><span class="o">)</span>
devfs on /dev <span class="o">(</span>devfs, <span class="nb">local</span><span class="o">)</span>
/dev/ad0s1d on /usr <span class="o">(</span>ufs, <span class="nb">local</span>, soft-updates<span class="o">)</span>
example on /example <span class="o">(</span>zfs, <span class="nb">local</span><span class="o">)</span>
example/compressed on /example/compressed <span class="o">(</span>zfs, <span class="nb">local</span><span class="o">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Use ZFS datasets like any file system after creation. Set other available features on a per-dataset basis when needed. The example below creates a new file system called <code>data</code>. It assumes the file system contains important files and configures it to store two copies of each data block.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs create example/data</span>
<span class="c"># zfs set copies=2 example/data</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Use <code>df</code> to see the data and space usage:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># df</span>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice that all file systems in the pool have the same available space. Using <code>df</code> in these examples shows that the file systems use the space they need and all draw from the same pool. ZFS gets rid of concepts such as volumes and partitions, and allows several file systems to share the same pool.</p>
</div>
<div class="paragraph">
<p>To destroy the file systems and then the pool that is no longer needed:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs destroy example/compressed</span>
<span class="c"># zfs destroy example/data</span>
<span class="c"># zpool destroy example</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-quickstart-raid-z">22.2.2. RAID-Z<a class="anchor" href="#zfs-quickstart-raid-z"></a></h3>
<div class="paragraph">
<p>Disks fail. One way to avoid data loss from disk failure is to use RAID. ZFS supports this feature in its pool design. RAID-Z pools require three or more disks but provide more usable space than mirrored pools.</p>
</div>
<div class="paragraph">
<p>This example creates a RAID-Z pool, specifying the disks to add to the pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create storage raidz da0 da1 da2</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Sun™ recommends that the number of devices used in a RAID-Z configuration be between three and nine. For environments requiring a single pool consisting of 10 disks or more, consider breaking it up into smaller RAID-Z groups. If two disks are available, ZFS mirroring provides redundancy if required. Refer to <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a> for more details.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>The previous example created the <code>storage</code> zpool. This example makes a new file system called <code>home</code> in that pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs create storage/home</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Enable compression and store an extra copy of directories and files:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set copies=2 storage/home</span>
<span class="c"># zfs set compression=gzip storage/home</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To make this the new home directory for users, copy the user data to this directory and create the appropriate symbolic links:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp -rp /home/* /storage/home</span>
<span class="c"># rm -rf /home /usr/home</span>
<span class="c"># ln -s /storage/home /home</span>
<span class="c"># ln -s /storage/home /usr/home</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Users data is now stored on the freshly-created <span class="filename">/storage/home</span>. Test by adding a new user and logging in as that user.</p>
</div>
<div class="paragraph">
<p>Create a file system snapshot to roll back to later:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs snapshot storage/home@08-30-08</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>ZFS creates snapshots of a dataset, not a single directory or file.</p>
</div>
<div class="paragraph">
<p>The <code>@</code> character is a delimiter between the file system name or the volume name. Before deleting an important directory, back up the file system, then roll back to an earlier snapshot in which the directory still exists:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs rollback storage/home@08-30-08</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To list all available snapshots, run <code>ls</code> in the file system’s <span class="filename">.zfs/snapshot</span> directory. For example, to see the snapshot taken:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># ls /storage/home/.zfs/snapshot</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Write a script to take regular snapshots of user data. Over time, snapshots can use up a lot of disk space. Remove the previous snapshot using the command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs destroy storage/home@08-30-08</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>After testing, make <span class="filename">/storage/home</span> the real <span class="filename">/home</span> with this command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set mountpoint=/home storage/home</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Run <code>df</code> and <code>mount</code> to confirm that the system now treats the file system as the real <span class="filename">/home</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># mount</span>
/dev/ad0s1a on / <span class="o">(</span>ufs, <span class="nb">local</span><span class="o">)</span>
devfs on /dev <span class="o">(</span>devfs, <span class="nb">local</span><span class="o">)</span>
/dev/ad0s1d on /usr <span class="o">(</span>ufs, <span class="nb">local</span>, soft-updates<span class="o">)</span>
storage on /storage <span class="o">(</span>zfs, <span class="nb">local</span><span class="o">)</span>
storage/home on /home <span class="o">(</span>zfs, <span class="nb">local</span><span class="o">)</span>
<span class="c"># df</span>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</code></pre>
</div>
</div>
<div class="paragraph">
<p>This completes the RAID-Z configuration. Add daily status updates about the created file systems to the nightly <a href="https://man.freebsd.org/cgi/man.cgi?query=periodic&amp;sektion=8&amp;format=html">periodic(8)</a> runs by adding this line to <span class="filename">/etc/periodic.conf</span>:</p>
</div>
<div class="literalblock programlisting">
<div class="content">
<pre>daily_status_zfs_enable=&#34;YES&#34;</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-quickstart-recovering-raid-z">22.2.3. Recovering RAID-Z<a class="anchor" href="#zfs-quickstart-recovering-raid-z"></a></h3>
<div class="paragraph">
<p>Every software RAID has a method of monitoring its <code>state</code>. View the status of RAID-Z devices using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status -x</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>If all pools are <a href="#zfs-term-online">Online</a> and everything is normal, the message shows:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell">all pools are healthy</code></pre>
</div>
</div>
<div class="paragraph">
<p>If there is a problem, perhaps a disk being in the <a href="#zfs-term-offline">Offline</a> state, the pool state will look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell">  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist <span class="k">for </span>the pool to <span class="k">continue </span>functioning <span class="k">in </span>a
	degraded state.
action: Online the device using <span class="s1">&#39;zpool online&#39;</span> or replace the device with
	<span class="s1">&#39;zpool replace&#39;</span>.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>&#34;OFFLINE&#34; shows the administrator took <span class="filename">da1</span> offline using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool offline storage da1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Power down the computer now and replace <span class="filename">da1</span>. Power up the computer and return <span class="filename">da1</span> to the pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool replace storage da1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, check the status again, this time without <code>-x</code> to display all pools:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, everything is normal.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-quickstart-data-verification">22.2.4. Data Verification<a class="anchor" href="#zfs-quickstart-data-verification"></a></h3>
<div class="paragraph">
<p>ZFS uses checksums to verify the integrity of stored data. Creating file systems automatically enables them.</p>
</div>
<div class="admonitionblock warning">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Disabling Checksums is possible but <em>not</em> recommended! Checksums take little storage space and provide data integrity. Most ZFS features will not work properly with checksums disabled. Disabling these checksums will not increase performance noticeably.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>Verifying the data checksums (called <em>scrubbing</em>) ensures integrity of the <code>storage</code> pool with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool scrub storage</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The duration of a scrub depends on the amount of data stored. Larger amounts of data will take proportionally longer to verify. Since scrubbing is I/O intensive, ZFS allows a single scrub to run at a time. After scrubbing completes, view the status with <code>zpool status</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status storage</span>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>Displaying the completion date of the last scrubbing helps decide when to start another. Routine scrubs help protect data from silent corruption and ensure the integrity of the pool.</p>
</div>
<div class="paragraph">
<p>Refer to <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=8&amp;format=html">zfs(8)</a> and <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a> for other ZFS options.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-zpool">22.3. <code>zpool</code> Administration<a class="anchor" href="#zfs-zpool"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>ZFS administration uses two main utilities. The <code>zpool</code> utility controls the operation of the pool and allows adding, removing, replacing, and managing disks. The <a href="#zfs-zfs"><code>zfs</code></a> utility allows creating, destroying, and managing datasets, both <a href="#zfs-term-filesystem">file systems</a> and <a href="#zfs-term-volume">volumes</a>.</p>
</div>
<div class="sect2">
<h3 id="zfs-zpool-create">22.3.1. Creating and Destroying Storage Pools<a class="anchor" href="#zfs-zpool-create"></a></h3>
<div class="paragraph">
<p>Creating a ZFS storage pool requires permanent decisions, as the pool structure cannot change after creation. The most important decision is which types of vdevs to group the physical disks into. See the list of <a href="#zfs-term-vdev">vdev types</a> for details about the possible options. After creating the pool, most vdev types do not allow adding disks to the vdev. The exceptions are mirrors, which allow adding new disks to the vdev, and stripes, which upgrade to mirrors by attaching a new disk to the vdev. Although adding new vdevs expands a pool, the pool layout cannot change after pool creation. Instead, back up the data, destroy the pool, and recreate it.</p>
</div>
<div class="paragraph">
<p>Create a simple mirror pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create mypool mirror /dev/ada1 /dev/ada2</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>To create more than one vdev with a single command, specify groups of disks separated by the vdev type keyword, <code>mirror</code> in this example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create mypool mirror /dev/ada1 /dev/ada2 mirror /dev/ada3 /dev/ada4</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada3    ONLINE       0     0     0
            ada4    ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>Pools can also use partitions rather than whole disks. Putting ZFS in a separate partition allows the same disk to have other partitions for other purposes. In particular, it allows adding partitions with bootcode and file systems needed for booting. This allows booting from disks that are also members of a pool. ZFS adds no performance penalty on FreeBSD when using a partition rather than a whole disk. Using partitions also allows the administrator to <em>under-provision</em> the disks, using less than the full capacity. If a future replacement disk of the same nominal size as the original actually has a slightly smaller capacity, the smaller partition will still fit, using the replacement disk.</p>
</div>
<div class="paragraph">
<p>Create a <a href="#zfs-term-vdev-raidz">RAID-Z2</a> pool using partitions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create mypool raidz2 /dev/ada0p3 /dev/ada1p3 /dev/ada2p3 /dev/ada3p3 /dev/ada4p3 /dev/ada5p3</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>Destroy a pool that is no longer needed to reuse the disks. Destroying a pool requires unmounting the file systems in that pool first. If any dataset is in use, the unmount operation fails without destroying the pool. Force the pool destruction with <code>-f</code>. This can cause undefined behavior in applications which had open files on those datasets.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-attach">22.3.2. Adding and Removing Devices<a class="anchor" href="#zfs-zpool-attach"></a></h3>
<div class="paragraph">
<p>Two ways exist for adding disks to a pool: attaching a disk to an existing vdev with <code>zpool attach</code>, or adding vdevs to the pool with <code>zpool add</code>. Some <a href="#zfs-term-vdev">vdev types</a> allow adding disks to the vdev after creation.</p>
</div>
<div class="paragraph">
<p>A pool created with a single disk lacks redundancy. It can detect corruption but can not repair it, because there is no other copy of the data. The <a href="#zfs-term-copies">copies</a> property may be able to recover from a small failure such as a bad sector, but does not provide the same level of protection as mirroring or RAID-Z. Starting with a pool consisting of a single disk vdev, use <code>zpool attach</code> to add a new disk to the vdev, creating a mirror. Also use <code>zpool attach</code> to add new disks to a mirror group, increasing redundancy and read performance. When partitioning the disks used for the pool, replicate the layout of the first disk on to the second. Use <code>gpart backup</code> and <code>gpart restore</code> to make this process easier.</p>
</div>
<div class="paragraph">
<p>Upgrade the single disk (stripe) vdev <span class="filename">ada0p3</span> to a mirror by attaching <span class="filename">ada1p3</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          ada0p3    ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool attach mypool ada0p3 ada1p3</span>
Make sure to <span class="nb">wait </span><span class="k">until </span>resilvering finishes before rebooting.

If you boot from pool <span class="s1">&#39;mypool&#39;</span>, you may need to update boot code on newly attached disk _ada1p3_.

Assuming you use GPT partitioning and _da0_ is your new boot disk you may use the following <span class="nb">command</span>:

        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0
<span class="c"># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span>
bootcode written to ada1
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class="k">continue </span>to <span class="k">function</span>, possibly <span class="k">in </span>a degraded state.
action: Wait <span class="k">for </span>the resilver to complete.
  scan: resilver <span class="k">in </span>progress since Fri May 30 08:19:19 2014
        527M scanned out of 781M at 47.9M/s, 0h0m to go
        527M resilvered, 67.53% <span class="k">done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0  <span class="o">(</span>resilvering<span class="o">)</span>

errors: No known data errors
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class="k">in </span>0h0m with 0 errors on Fri May 30 08:15:58 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>When adding disks to the existing vdev is not an option, as for RAID-Z, an alternative method is to add another vdev to the pool. Adding vdevs provides higher performance by distributing writes across the vdevs. Each vdev provides its own redundancy. Mixing vdev types like <code>mirror</code> and <code>RAID-Z</code> is possible but discouraged. Adding a non-redundant vdev to a pool containing mirror or RAID-Z vdevs risks the data on the entire pool. Distributing writes means a failure of the non-redundant disk will result in the loss of a fraction of every block written to the pool.</p>
</div>
<div class="paragraph">
<p>ZFS stripes data across each of the vdevs. For example, with two mirror vdevs, this is effectively a RAID 10 that stripes writes across two sets of mirrors. ZFS allocates space so that each vdev reaches 100% full at the same time. Having vdevs with different amounts of free space will lower performance, as more data writes go to the less full vdev.</p>
</div>
<div class="paragraph">
<p>When attaching new devices to a boot pool, remember to update the bootcode.</p>
</div>
<div class="paragraph">
<p>Attach a second mirror group (<span class="filename">ada2p3</span> and <span class="filename">ada3p3</span>) to the existing mirror:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class="k">in </span>0h0m with 0 errors on Fri May 30 08:19:35 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool add mypool mirror ada2p3 ada3p3</span>
<span class="c"># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
bootcode written to ada2
<span class="c"># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada3</span>
bootcode written to ada3
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class="k">in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>Removing vdevs from a pool is impossible and removal of disks from a mirror is exclusive if there is enough remaining redundancy. If a single disk remains in a mirror group, that group ceases to be a mirror and becomes a stripe, risking the entire pool if that remaining disk fails.</p>
</div>
<div class="paragraph">
<p>Remove a disk from a three-way mirror group:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class="k">in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool detach mypool ada2p3</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class="k">in </span>0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-status">22.3.3. Checking the Status of a Pool<a class="anchor" href="#zfs-zpool-status"></a></h3>
<div class="paragraph">
<p>Pool status is important. If a drive goes offline or ZFS detects a read, write, or checksum error, the corresponding error count increases. The <code>status</code> output shows the configuration and status of each device in the pool and the status of the entire pool. Actions to take and details about the last <a href="#zfs-zpool-scrub"><code>scrub</code></a> are also shown.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 <span class="k">in </span>2h25m with 0 errors on Sat Sep 14 04:25:50 2013
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-clear">22.3.4. Clearing Errors<a class="anchor" href="#zfs-zpool-clear"></a></h3>
<div class="paragraph">
<p>When detecting an error, ZFS increases the read, write, or checksum error counts. Clear the error message and reset the counts with <code>zpool clear <em>mypool</em></code>. Clearing the error state can be important for automated scripts that alert the administrator when the pool encounters an error. Without clearing old errors, the scripts may fail to report further errors.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-replace">22.3.5. Replacing a Functioning Device<a class="anchor" href="#zfs-zpool-replace"></a></h3>
<div class="paragraph">
<p>It may be desirable to replace one disk with a different disk. When replacing a working disk, the process keeps the old disk online during the replacement. The pool never enters a <a href="#zfs-term-degraded">degraded</a> state, reducing the risk of data loss. Running <code>zpool replace</code> copies the data from the old disk to the new one. After the operation completes, ZFS disconnects the old disk from the vdev. If the new disk is larger than the old disk, it may be possible to grow the zpool, using the new space. See <a href="#zfs-zpool-online">Growing a Pool</a>.</p>
</div>
<div class="paragraph">
<p>Replace a functioning device in the pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool replace mypool ada1p3 ada2p3</span>
Make sure to <span class="nb">wait </span><span class="k">until </span>resilvering finishes before rebooting.

When booting from the pool <span class="s1">&#39;zroot&#39;</span>, update the boot code on the newly attached disk <span class="s1">&#39;ada2p3&#39;</span>.

Assuming GPT partitioning is used and <span class="o">[</span>.filename]#da0# is the new boot disk, use the following <span class="nb">command</span>:

        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0
<span class="c"># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada2</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        <span class="k">continue </span>to <span class="k">function</span>, possibly <span class="k">in </span>a degraded state.
action: Wait <span class="k">for </span>the resilver to complete.
  scan: resilver <span class="k">in </span>progress since Mon Jun  2 14:21:35 2014
        604M scanned out of 781M at 46.5M/s, 0h0m to go
        604M resilvered, 77.39% <span class="k">done
</span>config:

        NAME             STATE     READ WRITE CKSUM
        mypool           ONLINE       0     0     0
          mirror-0       ONLINE       0     0     0
            ada0p3       ONLINE       0     0     0
            replacing-1  ONLINE       0     0     0
              ada1p3     ONLINE       0     0     0
              ada2p3     ONLINE       0     0     0  <span class="o">(</span>resilvering<span class="o">)</span>

errors: No known data errors
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class="k">in </span>0h0m with 0 errors on Mon Jun  2 14:21:52 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-resilver">22.3.6. Dealing with Failed Devices<a class="anchor" href="#zfs-zpool-resilver"></a></h3>
<div class="paragraph">
<p>When a disk in a pool fails, the vdev to which the disk belongs enters the <a href="#zfs-term-degraded">degraded</a> state. The data is still available, but with reduced performance because ZFS computes missing data from the available redundancy. To restore the vdev to a fully functional state, replace the failed physical device. ZFS is then instructed to begin the <a href="#zfs-term-resilver">resilver</a> operation. ZFS recomputes data on the failed device from available redundancy and writes it to the replacement device. After completion, the vdev returns to <a href="#zfs-term-online">online</a> status.</p>
</div>
<div class="paragraph">
<p>If the vdev does not have any redundancy, or if devices have failed and there is not enough redundancy to compensate, the pool enters the <a href="#zfs-term-faulted">faulted</a> state. Unless enough devices can reconnect the pool becomes inoperative requiring a data restore from backups.</p>
</div>
<div class="paragraph">
<p>When replacing a failed disk, the name of the failed disk changes to the GUID of the new disk. A new device name parameter for <code>zpool replace</code> is not required if the replacement device has the same device name.</p>
</div>
<div class="paragraph">
<p>Replace a failed disk using <code>zpool replace</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist <span class="k">for
        </span>the pool to <span class="k">continue </span>functioning <span class="k">in </span>a degraded state.
action: Attach the missing device and online it using <span class="s1">&#39;zpool online&#39;</span>.
   see: http://illumos.org/msg/ZFS-8000-2Q
  scan: none requested
config:

        NAME                    STATE     READ WRITE CKSUM
        mypool                  DEGRADED     0     0     0
          mirror-0              DEGRADED     0     0     0
            ada0p3              ONLINE       0     0     0
            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3

errors: No known data errors
<span class="c"># zpool replace mypool 316502962686821739 ada2p3</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
        <span class="k">continue </span>to <span class="k">function</span>, possibly <span class="k">in </span>a degraded state.
action: Wait <span class="k">for </span>the resilver to complete.
  scan: resilver <span class="k">in </span>progress since Mon Jun  2 14:52:21 2014
        641M scanned out of 781M at 49.3M/s, 0h0m to go
        640M resilvered, 82.04% <span class="k">done
</span>config:

        NAME                        STATE     READ WRITE CKSUM
        mypool                      DEGRADED     0     0     0
          mirror-0                  DEGRADED     0     0     0
            ada0p3                  ONLINE       0     0     0
            replacing-1             UNAVAIL      0     0     0
              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old
              ada2p3                ONLINE       0     0     0  <span class="o">(</span>resilvering<span class="o">)</span>

errors: No known data errors
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M <span class="k">in </span>0h0m with 0 errors on Mon Jun  2 14:52:38 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-scrub">22.3.7. Scrubbing a Pool<a class="anchor" href="#zfs-zpool-scrub"></a></h3>
<div class="paragraph">
<p>Routinely <a href="#zfs-term-scrub">scrub</a> pools, ideally at least once every month. The <code>scrub</code> operation is disk-intensive and will reduce performance while running. Avoid high-demand periods when scheduling <code>scrub</code> or use <a href="#zfs-advanced-tuning-scrub_delay"><code>vfs.zfs.scrub_delay</code></a> to adjust the relative priority of the <code>scrub</code> to keep it from slowing down other workloads.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool scrub mypool</span>
<span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
  scan: scrub <span class="k">in </span>progress since Wed Feb 19 20:52:54 2014
        116G scanned out of 8.60T at 649M/s, 3h48m to go
        0 repaired, 1.32% <span class="k">done
</span>config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>To cancel a scrub operation if needed, run <code>zpool scrub -s <em>mypool</em></code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-selfheal">22.3.8. Self-Healing<a class="anchor" href="#zfs-zpool-selfheal"></a></h3>
<div class="paragraph">
<p>The checksums stored with data blocks enable the file system to <em>self-heal</em>. This feature will automatically repair data whose checksum does not match the one recorded on another device that is part of the storage pool. For example, a mirror configuration with two disks where one drive is starting to malfunction and cannot properly store the data any more. This is worse when the data was not accessed for a long time, as with long term archive storage. Traditional file systems need to run commands that check and repair the data like <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a>. These commands take time, and in severe cases, an administrator has to decide which repair operation to perform. When ZFS detects a data block with a mismatched checksum, it tries to read the data from the mirror disk. If that disk can provide the correct data, ZFS will give that to the application and correct the data on the disk with the wrong checksum. This happens without any interaction from a system administrator during normal pool operation.</p>
</div>
<div class="paragraph">
<p>The next example shows this self-healing behavior by creating a mirrored pool of disks <span class="filename">/dev/ada0</span> and <span class="filename">/dev/ada1</span>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool create healer mirror /dev/ada0 /dev/ada1</span>
<span class="c"># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool list</span>
NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Copy some important data to the pool to protect from data errors using the self-healing feature and create a checksum of the pool for later comparison.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp /some/important/data /healer</span>
<span class="c"># zfs list</span>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  67.7M   892M     7%  1.00x  ONLINE  -
<span class="c"># sha1 /healer &gt; checksum.txt</span>
<span class="c"># cat checksum.txt</span>
SHA1 <span class="o">(</span>/healer<span class="o">)</span> <span class="o">=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre>
</div>
</div>
<div class="paragraph">
<p>Simulate data corruption by writing random data to the beginning of one of the disks in the mirror. To keep ZFS from healing the data when detected, export the pool before the corruption and import it again afterwards.</p>
</div>
<div class="admonitionblock warning">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>This is a dangerous operation that can destroy vital data, shown here for demonstration alone. <strong>Do not try</strong> it during normal operation of a storage pool. Nor should this intentional corruption example run on any disk with a file system not using ZFS on another partition in it. Do not use any other disk device names other than the ones that are part of the pool. Ensure proper backups of the pool exist and test them before running the command!</p>
</div>
</td>
</tr>
</tbody></table>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool export healer</span>
<span class="c"># dd if=/dev/random of=/dev/ada1 bs=1m count=200</span>
200+0 records <span class="k">in
</span>200+0 records out
209715200 bytes transferred <span class="k">in </span>62.992162 secs <span class="o">(</span>3329227 bytes/sec<span class="o">)</span>
<span class="c"># zpool import healer</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The pool status shows that one device has experienced an error. Note that applications reading data from the pool did not receive any incorrect data. ZFS provided data from the <span class="filename">ada0</span> device with the correct checksums. To find the device with the wrong checksum, look for one whose <code>CKSUM</code> column contains a nonzero value.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status healer</span>
    pool: healer
   state: ONLINE
  status: One or more devices has experienced an unrecoverable error.  An
          attempt was made to correct the error.  Applications are unaffected.
  action: Determine <span class="k">if </span>the device needs to be replaced, and clear the errors
          using <span class="s1">&#39;zpool clear&#39;</span> or replace the device with <span class="s1">&#39;zpool replace&#39;</span>.
     see: http://illumos.org/msg/ZFS-8000-4J
    scan: none requested
  config:

      NAME        STATE     READ WRITE CKSUM
      healer      ONLINE       0     0     0
        mirror-0  ONLINE       0     0     0
         ada0     ONLINE       0     0     0
         ada1     ONLINE       0     0     1

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>ZFS detected the error and handled it by using the redundancy present in the unaffected <span class="filename">ada0</span> mirror disk. A checksum comparison with the original one will reveal whether the pool is consistent again.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># sha1 /healer &gt;&gt; checksum.txt</span>
<span class="c"># cat checksum.txt</span>
SHA1 <span class="o">(</span>/healer<span class="o">)</span> <span class="o">=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f
SHA1 <span class="o">(</span>/healer<span class="o">)</span> <span class="o">=</span> 2753eff56d77d9a536ece6694bf0a82740344d1f</code></pre>
</div>
</div>
<div class="paragraph">
<p>Generate checksums before and after the intentional tampering while the pool data still matches. This shows how ZFS is capable of detecting and correcting any errors automatically when the checksums differ. Note this is possible with enough redundancy present in the pool. A pool consisting of a single device has no self-healing capabilities. That is also the reason why checksums are so important in ZFS; do not disable them for any reason. ZFS requires no <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> or similar file system consistency check program to detect and correct this, and keeps the pool available while there is a problem. A scrub operation is now required to overwrite the corrupted data on <span class="filename">ada1</span>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool scrub healer</span>
<span class="c"># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
            attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class="k">if </span>the device needs to be replaced, and clear the errors
            using <span class="s1">&#39;zpool clear&#39;</span> or replace the device with <span class="s1">&#39;zpool replace&#39;</span>.
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub <span class="k">in </span>progress since Mon Dec 10 12:23:30 2012
        10.4M scanned out of 67.0M at 267K/s, 0h3m to go
        9.63M repaired, 15.56% <span class="k">done
</span>config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0   627  <span class="o">(</span>repairing<span class="o">)</span>

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>The scrub operation reads data from <span class="filename">ada0</span> and rewrites any data with a wrong checksum on <span class="filename">ada1</span>, shown by the <code>(repairing)</code> output from <code>zpool status</code>. After the operation is complete, the pool status changes to:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status healer</span>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine <span class="k">if </span>the device needs to be replaced, and clear the errors
             using <span class="s1">&#39;zpool clear&#39;</span> or replace the device with <span class="s1">&#39;zpool replace&#39;</span>.
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub repaired 66.5M <span class="k">in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0 2.72K

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>After the scrubbing operation completes with all the data synchronized from <span class="filename">ada0</span> to <span class="filename">ada1</span>, <a href="#zfs-zpool-clear">clear</a> the error messages from the pool status by running <code>zpool clear</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool clear healer</span>
<span class="c"># zpool status healer</span>
  pool: healer
 state: ONLINE
  scan: scrub repaired 66.5M <span class="k">in </span>0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors</code></pre>
</div>
</div>
<div class="paragraph">
<p>The pool is now back to a fully working state, with all error counts now zero.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-online">22.3.9. Growing a Pool<a class="anchor" href="#zfs-zpool-online"></a></h3>
<div class="paragraph">
<p>The smallest device in each vdev limits the usable size of a redundant pool. Replace the smallest device with a larger device. After completing a <a href="#zfs-zpool-replace">replace</a> or <a href="#zfs-term-resilver">resilver</a> operation, the pool can grow to use the capacity of the new device. For example, consider a mirror of a 1 TB drive and a 2 TB drive. The usable space is 1 TB. When replacing the 1 TB drive with another 2 TB drive, the resilvering process copies the existing data onto the new drive. As both of the devices now have 2 TB capacity, the mirror’s available space grows to 2 TB.</p>
</div>
<div class="paragraph">
<p>Start expansion by using <code>zpool online -e</code> on each device. After expanding all devices, the extra space becomes available to the pool.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-import">22.3.10. Importing and Exporting Pools<a class="anchor" href="#zfs-zpool-import"></a></h3>
<div class="paragraph">
<p><em>Export</em> pools before moving them to another system. ZFS unmounts all datasets, marking each device as exported but still locked to prevent use by other disks. This allows pools to be <em>imported</em> on other machines, other operating systems that support ZFS, and even different hardware architectures (with some caveats, see <a href="https://man.freebsd.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;format=html">zpool(8)</a>). When a dataset has open files, use <code>zpool export -f</code> to force exporting the pool. Use this with caution. The datasets are forcibly unmounted, potentially resulting in unexpected behavior by the applications which had open files on those datasets.</p>
</div>
<div class="paragraph">
<p>Export a pool that is not in use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool export mypool</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Importing a pool automatically mounts the datasets. If this is undesired behavior, use <code>zpool import -N</code> to prevent it. <code>zpool import -o</code> sets temporary properties for this specific import. <code>zpool import altroot=</code> allows importing a pool with a base mount point instead of the root of the file system. If the pool was last used on a different system and was not properly exported, force the import using <code>zpool import -f</code>. <code>zpool import -a</code> imports all pools that do not appear to be in use by another system.</p>
</div>
<div class="paragraph">
<p>List all available pools for import:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool import</span>
   pool: mypool
     id: 9930174748043525076
  state: ONLINE
 action: The pool can be imported using its name or numeric identifier.
 config:

        mypool      ONLINE
          ada2p3    ONLINE</code></pre>
</div>
</div>
<div class="paragraph">
<p>Import the pool with an alternative root directory:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool import -o altroot=/mnt mypool</span>
<span class="c"># zfs list</span>
zfs list
NAME                 USED  AVAIL  REFER  MOUNTPOINT
mypool               110K  47.0G    31K  /mnt/mypool</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-upgrade">22.3.11. Upgrading a Storage Pool<a class="anchor" href="#zfs-zpool-upgrade"></a></h3>
<div class="paragraph">
<p>After upgrading FreeBSD, or if importing a pool from a system using an older version, manually upgrade the pool to the latest ZFS version to support newer features. Consider whether the pool may ever need importing on an older system before upgrading. Upgrading is a one-way process. Upgrade older pools is possible, but downgrading pools with newer features is not.</p>
</div>
<div class="paragraph">
<p>Upgrade a v28 pool to support <code>Feature Flags</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
status: The pool is formatted using a legacy on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using <span class="s1">&#39;zpool upgrade&#39;</span>.  Once this is <span class="k">done</span>, the
        pool will no longer be accessible on software that does not support feat
        flags.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool upgrade</span>
This system supports ZFS pool feature flags.

The following pools are formatted with legacy version numbers and are upgraded to use feature flags.
After being upgraded, these pools will no longer be accessible by software that does not support feature flags.

VER  POOL
---  ------------
28   mypool

Use <span class="s1">&#39;zpool upgrade -v&#39;</span> <span class="k">for </span>a list of available legacy versions.
Every feature flags pool has all supported features enabled.
<span class="c"># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Successfully upgraded <span class="s1">&#39;mypool&#39;</span> from version 28 to feature flags.
Enabled the following features on <span class="s1">&#39;mypool&#39;</span>:
  async_destroy
  empty_bpobj
  lz4_compress
  multi_vdev_crash_dump</code></pre>
</div>
</div>
<div class="paragraph">
<p>The newer features of ZFS will not be available until <code>zpool upgrade</code> has completed. Use <code>zpool upgrade -v</code> to see what new features the upgrade provides, as well as which features are already supported.</p>
</div>
<div class="paragraph">
<p>Upgrade a pool to support new feature flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool status</span>
  pool: mypool
 state: ONLINE
status: Some supported features are not enabled on the pool. The pool can
        still be used, but some features are unavailable.
action: Enable all features using <span class="s1">&#39;zpool upgrade&#39;</span>. Once this is <span class="k">done</span>,
        the pool may no longer be accessible by software that does not support
        the features. See zpool-features<span class="o">(</span>7<span class="o">)</span> <span class="k">for </span>details.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<span class="c"># zpool upgrade</span>
This system supports ZFS pool feature flags.

All pools are formatted using feature flags.

Some supported features are not enabled on the following pools. Once a
feature is enabled the pool may become incompatible with software
that does not support the feature. See zpool-features<span class="o">(</span>7<span class="o">)</span> <span class="k">for </span>details.

POOL  FEATURE
---------------
zstore
      multi_vdev_crash_dump
      spacemap_histogram
      enabled_txg
      hole_birth
      extensible_dataset
      bookmarks
      filesystem_limits
<span class="c"># zpool upgrade mypool</span>
This system supports ZFS pool feature flags.

Enabled the following features on <span class="s1">&#39;mypool&#39;</span>:
  spacemap_histogram
  enabled_txg
  hole_birth
  extensible_dataset
  bookmarks
  filesystem_limits</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Update the boot code on systems that boot from a pool to support the new pool version. Use <code>gpart bootcode</code> on the partition that contains the boot code. Two types of bootcode are available, depending on way the system boots: GPT (the most common option) and EFI (for more modern systems).</p>
</div>
<div class="paragraph">
<p>For legacy boot using GPT, use the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 ada1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>For systems using EFI to boot, execute the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># gpart bootcode -p /boot/boot1.efifat -i 1 ada1</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Apply the bootcode to all bootable disks in the pool. See <a href="https://man.freebsd.org/cgi/man.cgi?query=gpart&amp;sektion=8&amp;format=html">gpart(8)</a> for more information.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-history">22.3.12. Displaying Recorded Pool History<a class="anchor" href="#zfs-zpool-history"></a></h3>
<div class="paragraph">
<p>ZFS records commands that change the pool, including creating datasets, changing properties, or replacing a disk. Reviewing history about a pool’s creation is useful, as is checking which user performed a specific action and when. History is not kept in a log file, but is part of the pool itself. The command to review this history is aptly named <code>zpool history</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool history</span>
History <span class="k">for</span> <span class="s1">&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1
2013-02-27.18:50:58 zfs <span class="nb">set </span><span class="nv">atime</span><span class="o">=</span>off tank
2013-02-27.18:51:09 zfs <span class="nb">set </span><span class="nv">checksum</span><span class="o">=</span>fletcher4 tank
2013-02-27.18:51:18 zfs create tank/backup</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output shows <code>zpool</code> and <code>zfs</code> commands altering the pool in some way along with a timestamp. Commands like <code>zfs list</code> are not included. When specifying no pool name, ZFS displays history of all pools.</p>
</div>
<div class="paragraph">
<p><code>zpool history</code> can show even more information when providing the options <code>-i</code> or <code>-l</code>. <code>-i</code> displays user-initiated events as well as internally logged ZFS events.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool history -i</span>
History <span class="k">for</span> <span class="s1">&#39;tank&#39;</span>:
2013-02-26.23:02:35 <span class="o">[</span>internal pool create txg:5] pool spa 28; zfs spa 28; zpl 5;uts  9.1-RELEASE 901000 amd64
2013-02-27.18:50:53 <span class="o">[</span>internal property <span class="nb">set </span>txg:50] <span class="nv">atime</span><span class="o">=</span>0 dataset <span class="o">=</span> 21
2013-02-27.18:50:58 zfs <span class="nb">set </span><span class="nv">atime</span><span class="o">=</span>off tank
2013-02-27.18:51:04 <span class="o">[</span>internal property <span class="nb">set </span>txg:53] <span class="nv">checksum</span><span class="o">=</span>7 dataset <span class="o">=</span> 21
2013-02-27.18:51:09 zfs <span class="nb">set </span><span class="nv">checksum</span><span class="o">=</span>fletcher4 tank
2013-02-27.18:51:13 <span class="o">[</span>internal create txg:55] dataset <span class="o">=</span> 39
2013-02-27.18:51:18 zfs create tank/backup</code></pre>
</div>
</div>
<div class="paragraph">
<p>Show more details by adding <code>-l</code>. Showing history records in a long format, including information like the name of the user who issued the command and the hostname on which the change happened.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool history -l</span>
History <span class="k">for</span> <span class="s1">&#39;tank&#39;</span>:
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 <span class="o">[</span>user 0 <span class="o">(</span>root<span class="o">)</span> on :global]
2013-02-27.18:50:58 zfs <span class="nb">set </span><span class="nv">atime</span><span class="o">=</span>off tank <span class="o">[</span>user 0 <span class="o">(</span>root<span class="o">)</span> on myzfsbox:global]
2013-02-27.18:51:09 zfs <span class="nb">set </span><span class="nv">checksum</span><span class="o">=</span>fletcher4 tank <span class="o">[</span>user 0 <span class="o">(</span>root<span class="o">)</span> on myzfsbox:global]
2013-02-27.18:51:18 zfs create tank/backup <span class="o">[</span>user 0 <span class="o">(</span>root<span class="o">)</span> on myzfsbox:global]</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output shows that the <code>root</code> user created the mirrored pool with disks <span class="filename">/dev/ada0</span> and <span class="filename">/dev/ada1</span>. The hostname <code>myzfsbox</code> is also shown in the commands after the pool’s creation. The hostname display becomes important when exporting the pool from one system and importing on another. It’s possible to distinguish the commands issued on the other system by the hostname recorded for each command.</p>
</div>
<div class="paragraph">
<p>Combine both options to <code>zpool history</code> to give the most detailed information possible for any given pool. Pool history provides valuable information when tracking down the actions performed or when needing more detailed output for debugging.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-iostat">22.3.13. Performance Monitoring<a class="anchor" href="#zfs-zpool-iostat"></a></h3>
<div class="paragraph">
<p>A built-in monitoring system can display pool I/O statistics in real time. It shows the amount of free and used space on the pool, read and write operations performed per second, and I/O bandwidth used. By default, ZFS monitors and displays all pools in the system. Provide a pool name to limit monitoring to that pool. A basic example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool iostat</span>
               capacity     operations    bandwidth
pool        alloc   free   <span class="nb">read  </span>write   <span class="nb">read  </span>write
----------  -----  -----  -----  -----  -----  -----
data         288G  1.53T      2     11  11.3K  57.1K</code></pre>
</div>
</div>
<div class="paragraph">
<p>To continuously see I/O activity, specify a number as the last parameter, indicating an interval in seconds to wait between updates. The next statistic line prints after each interval. Press <span class="keyseq"><kbd>Ctrl</kbd>+<kbd>C</kbd></span> to stop this continuous monitoring. Give a second number on the command line after the interval to specify the total number of statistics to display.</p>
</div>
<div class="paragraph">
<p>Display even more detailed I/O statistics with <code>-v</code>. Each device in the pool appears with a statistics line. This is useful for seeing read and write operations performed on each device, and can help determine if any individual device is slowing down the pool. This example shows a mirrored pool with two devices:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool iostat -v</span>
                            capacity     operations    bandwidth
pool                     alloc   free   <span class="nb">read  </span>write   <span class="nb">read  </span>write
-----------------------  -----  -----  -----  -----  -----  -----
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
-----------------------  -----  -----  -----  -----  -----  -----</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zpool-split">22.3.14. Splitting a Storage Pool<a class="anchor" href="#zfs-zpool-split"></a></h3>
<div class="paragraph">
<p>ZFS can split a pool consisting of one or more mirror vdevs into two pools. Unless otherwise specified, ZFS detaches the last member of each mirror and creates a new pool containing the same data. Be sure to make a dry run of the operation with <code>-n</code> first. This displays the details of the requested operation without actually performing it. This helps confirm that the operation will do what the user intends.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-zfs">22.4. <code>zfs</code> Administration<a class="anchor" href="#zfs-zfs"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <code>zfs</code> utility can create, destroy, and manage all existing ZFS datasets within a pool. To manage the pool itself, use <a href="#zfs-zpool"><code>zpool</code></a>.</p>
</div>
<div class="sect2">
<h3 id="zfs-zfs-create">22.4.1. Creating and Destroying Datasets<a class="anchor" href="#zfs-zfs-create"></a></h3>
<div class="paragraph">
<p>Unlike traditional disks and volume managers, space in ZFS is <em>not</em> preallocated. With traditional file systems, after partitioning and assigning the space, there is no way to add a new file system without adding a new disk. With ZFS, creating new file systems is possible at any time. Each <a href="#zfs-term-dataset"><em>dataset</em></a> has properties including features like compression, deduplication, caching, and quotas, as well as other useful properties like readonly, case sensitivity, network file sharing, and a mount point. Nesting datasets within each other is possible and child datasets will inherit properties from their ancestors. <a href="#zfs-zfs-allow">Delegate</a>, <a href="#zfs-zfs-send">replicate</a>, <a href="#zfs-zfs-snapshot">snapshot</a>, <a href="#zfs-zfs-jail">jail</a> allows administering and destroying each dataset as a unit. Creating a separate dataset for each different type or set of files has advantages. The drawbacks to having a large number of datasets are that some commands like <code>zfs list</code> will be slower, and that mounting of hundreds or even thousands of datasets will slow the FreeBSD boot process.</p>
</div>
<div class="paragraph">
<p>Create a new dataset and enable <a href="#zfs-term-compression-lz4">LZ4 compression</a> on it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.20M  93.2G   608K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp
<span class="c"># zfs create -o compress=lz4 mypool/usr/mydataset</span>
<span class="c"># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 781M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.20M  93.2G   610K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp</code></pre>
</div>
</div>
<div class="paragraph">
<p>Destroying a dataset is much quicker than deleting the files on the dataset, as it does not involve scanning the files and updating the corresponding metadata.</p>
</div>
<div class="paragraph">
<p>Destroy the created dataset:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 880M  93.1G   144K  none
mypool/ROOT            777M  93.1G   144K  none
mypool/ROOT/default    777M  93.1G   777M  /
mypool/tmp             176K  93.1G   176K  /tmp
mypool/usr             101M  93.1G   144K  /usr
mypool/usr/home        184K  93.1G   184K  /usr/home
mypool/usr/mydataset   100M  93.1G   100M  /usr/mydataset
mypool/usr/ports       144K  93.1G   144K  /usr/ports
mypool/usr/src         144K  93.1G   144K  /usr/src
mypool/var            1.20M  93.1G   610K  /var
mypool/var/crash       148K  93.1G   148K  /var/crash
mypool/var/log         178K  93.1G   178K  /var/log
mypool/var/mail        144K  93.1G   144K  /var/mail
mypool/var/tmp         152K  93.1G   152K  /var/tmp
<span class="c"># zfs destroy mypool/usr/mydataset</span>
<span class="c"># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                781M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.21M  93.2G   612K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre>
</div>
</div>
<div class="paragraph">
<p>In modern versions of ZFS, <code>zfs destroy</code> is asynchronous, and the free space might take minutes to appear in the pool. Use <code>zpool get freeing <em>poolname</em></code> to see the <code>freeing</code> property, that shows which datasets are having their blocks freed in the background. If there are child datasets, like <a href="#zfs-term-snapshot">snapshots</a> or other datasets, destroying the parent is impossible. To destroy a dataset and its children, use <code>-r</code> to recursively destroy the dataset and its children. Use <code>-n -v</code> to list datasets and snapshots destroyed by this operation, without actually destroy anything. Space reclaimed by destroying snapshots is also shown.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-volume">22.4.2. Creating and Destroying Volumes<a class="anchor" href="#zfs-zfs-volume"></a></h3>
<div class="paragraph">
<p>A volume is a special dataset type. Rather than mounting as a file system, expose it as a block device under <span class="filename">/dev/zvol/poolname/dataset</span>. This allows using the volume for other file systems, to back the disks of a virtual machine, or to make it available to other network hosts using protocols like iSCSI or HAST.</p>
</div>
<div class="paragraph">
<p>Format a volume with any file system or without a file system to store raw data. To the user, a volume appears to be a regular disk. Putting ordinary file systems on these <em>zvols</em> provides features that ordinary disks or file systems do not have. For example, using the compression property on a 250 MB volume allows creation of a compressed FAT file system.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs create -V 250m -o compression=on tank/fat32</span>
<span class="c"># zfs list tank</span>
NAME USED AVAIL REFER MOUNTPOINT
tank 258M  670M   31K /tank
<span class="c"># newfs_msdos -F32 /dev/zvol/tank/fat32</span>
<span class="c"># mount -t msdosfs /dev/zvol/tank/fat32 /mnt</span>
<span class="c"># df -h /mnt | grep fat32</span>
Filesystem           Size Used Avail Capacity Mounted on
/dev/zvol/tank/fat32 249M  24k  249M     0%   /mnt
<span class="c"># mount | grep fat32</span>
/dev/zvol/tank/fat32 on /mnt <span class="o">(</span>msdosfs, <span class="nb">local</span><span class="o">)</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Destroying a volume is much the same as destroying a regular file system dataset. The operation is nearly instantaneous, but it may take minutes to reclaim the free space in the background.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-rename">22.4.3. Renaming a Dataset<a class="anchor" href="#zfs-zfs-rename"></a></h3>
<div class="paragraph">
<p>To change the name of a dataset, use <code>zfs rename</code>. To change the parent of a dataset, use this command as well. Renaming a dataset to have a different parent dataset will change the value of those properties inherited from the parent dataset. Renaming a dataset unmounts then remounts it in the new location (inherited from the new parent dataset). To prevent this behavior, use <code>-u</code>.</p>
</div>
<div class="paragraph">
<p>Rename a dataset and move it to be under a different parent dataset:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list</span>
NAME                   USED  AVAIL  REFER  MOUNTPOINT
mypool                 780M  93.2G   144K  none
mypool/ROOT            777M  93.2G   144K  none
mypool/ROOT/default    777M  93.2G   777M  /
mypool/tmp             176K  93.2G   176K  /tmp
mypool/usr             704K  93.2G   144K  /usr
mypool/usr/home        184K  93.2G   184K  /usr/home
mypool/usr/mydataset  87.5K  93.2G  87.5K  /usr/mydataset
mypool/usr/ports       144K  93.2G   144K  /usr/ports
mypool/usr/src         144K  93.2G   144K  /usr/src
mypool/var            1.21M  93.2G   614K  /var
mypool/var/crash       148K  93.2G   148K  /var/crash
mypool/var/log         178K  93.2G   178K  /var/log
mypool/var/mail        144K  93.2G   144K  /var/mail
mypool/var/tmp         152K  93.2G   152K  /var/tmp
<span class="c"># zfs rename mypool/usr/mydataset mypool/var/newname</span>
<span class="c"># zfs list</span>
NAME                  USED  AVAIL  REFER  MOUNTPOINT
mypool                780M  93.2G   144K  none
mypool/ROOT           777M  93.2G   144K  none
mypool/ROOT/default   777M  93.2G   777M  /
mypool/tmp            176K  93.2G   176K  /tmp
mypool/usr            616K  93.2G   144K  /usr
mypool/usr/home       184K  93.2G   184K  /usr/home
mypool/usr/ports      144K  93.2G   144K  /usr/ports
mypool/usr/src        144K  93.2G   144K  /usr/src
mypool/var           1.29M  93.2G   614K  /var
mypool/var/crash      148K  93.2G   148K  /var/crash
mypool/var/log        178K  93.2G   178K  /var/log
mypool/var/mail       144K  93.2G   144K  /var/mail
mypool/var/newname   87.5K  93.2G  87.5K  /var/newname
mypool/var/tmp        152K  93.2G   152K  /var/tmp</code></pre>
</div>
</div>
<div class="paragraph">
<p>Renaming snapshots uses the same command. Due to the nature of snapshots, rename cannot change their parent dataset. To rename a recursive snapshot, specify <code>-r</code>; this will also rename all snapshots with the same name in child datasets.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -t snapshot</span>
NAME                                USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@first_snapshot      0      -  87.5K  -
<span class="c"># zfs rename mypool/var/newname@first_snapshot new_snapshot_name</span>
<span class="c"># zfs list -t snapshot</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/newname@new_snapshot_name      0      -  87.5K  -</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-set">22.4.4. Setting Dataset Properties<a class="anchor" href="#zfs-zfs-set"></a></h3>
<div class="paragraph">
<p>Each ZFS dataset has properties that control its behavior. Most properties are automatically inherited from the parent dataset, but can be overridden locally. Set a property on a dataset with <code>zfs set <em>property=value dataset</em></code>. Most properties have a limited set of valid values, <code>zfs get</code> will display each possible property and valid values. Using <code>zfs inherit</code> reverts most properties to their inherited values. User-defined properties are also possible. They become part of the dataset configuration and provide further information about the dataset or its contents. To distinguish these custom properties from the ones supplied as part of ZFS, use a colon (<code>:</code>) to create a custom namespace for the property.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set custom:costcenter=1234 tank</span>
<span class="c"># zfs get custom:costcenter tank</span>
NAME PROPERTY           VALUE SOURCE
tank custom:costcenter  1234  <span class="nb">local</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove a custom property, use <code>zfs inherit</code> with <code>-r</code>. If the custom property is not defined in any of the parent datasets, this option removes it (but the pool’s history still records the change).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs inherit -r custom:costcenter tank</span>
<span class="c"># zfs get custom:costcenter tank</span>
NAME    PROPERTY           VALUE              SOURCE
tank    custom:costcenter  -                  -
<span class="c"># zfs get all tank | grep custom:costcenter</span>
<span class="c">#</span></code></pre>
</div>
</div>
<div class="sect3">
<h4 id="zfs-zfs-set-share">22.4.4.1. Getting and Setting Share Properties<a class="anchor" href="#zfs-zfs-set-share"></a></h4>
<div class="paragraph">
<p>Two commonly used and useful dataset properties are the NFS and SMB share options. Setting these defines if and how ZFS shares datasets on the network. At present, FreeBSD supports setting NFS sharing alone. To get the current status of a share, enter:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get sharenfs mypool/usr/home</span>
NAME             PROPERTY  VALUE    SOURCE
mypool/usr/home  sharenfs  on       <span class="nb">local</span>
<span class="c"># zfs get sharesmb mypool/usr/home</span>
NAME             PROPERTY  VALUE    SOURCE
mypool/usr/home  sharesmb  off      <span class="nb">local</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To enable sharing of a dataset, enter:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c">#  zfs set sharenfs=on mypool/usr/home</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Set other options for sharing datasets through NFS, such as <code>-alldirs</code>, <code>-maproot</code> and <code>-network</code>. To set options on a dataset shared through NFS, enter:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c">#  zfs set sharenfs=&#34;-alldirs,-maproot=root,-network=192.168.1.0/24&#34; mypool/usr/home</span></code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-snapshot">22.4.5. Managing Snapshots<a class="anchor" href="#zfs-zfs-snapshot"></a></h3>
<div class="paragraph">
<p><a href="#zfs-term-snapshot">Snapshots</a> are one of the most powerful features of ZFS. A snapshot provides a read-only, point-in-time copy of the dataset. With Copy-On-Write (COW), ZFS creates snapshots fast by preserving older versions of the data on disk. If no snapshots exist, ZFS reclaims space for future use when data is rewritten or deleted. Snapshots preserve disk space by recording just the differences between the current dataset and a previous version. Allowing snapshots on whole datasets, not on individual files or directories. A snapshot from a dataset duplicates everything contained in it. This includes the file system properties, files, directories, permissions, and so on. Snapshots use no extra space when first created, but consume space as the blocks they reference change. Recursive snapshots taken with <code>-r</code> create snapshots with the same name on the dataset and its children, providing a consistent moment-in-time snapshot of the file systems. This can be important when an application has files on related datasets or that depend upon each other. Without snapshots, a backup would have copies of the files from different points in time.</p>
</div>
<div class="paragraph">
<p>Snapshots in ZFS provide a variety of features that even other file systems with snapshot functionality lack. A typical example of snapshot use is as a quick way of backing up the current state of the file system when performing a risky action like a software installation or a system upgrade. If the action fails, rolling back to the snapshot returns the system to the same state when creating the snapshot. If the upgrade was successful, delete the snapshot to free up space. Without snapshots, a failed upgrade often requires restoring backups, which is tedious, time consuming, and may require downtime during which the system is unusable. Rolling back to snapshots is fast, even while the system is running in normal operation, with little or no downtime. The time savings are enormous with multi-terabyte storage systems considering the time required to copy the data from backup. Snapshots are not a replacement for a complete backup of a pool, but offer a quick and easy way to store a dataset copy at a specific time.</p>
</div>
<div class="sect3">
<h4 id="zfs-zfs-snapshot-creation">22.4.5.1. Creating Snapshots<a class="anchor" href="#zfs-zfs-snapshot-creation"></a></h4>
<div class="paragraph">
<p>To create snapshots, use <code>zfs snapshot <em>dataset</em>@<em>snapshotname</em></code>. Adding <code>-r</code> creates a snapshot recursively, with the same name on all child datasets.</p>
</div>
<div class="paragraph">
<p>Create a recursive snapshot of the entire pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -t all</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool                                 780M  93.2G   144K  none
mypool/ROOT                            777M  93.2G   144K  none
mypool/ROOT/default                    777M  93.2G   777M  /
mypool/tmp                             176K  93.2G   176K  /tmp
mypool/usr                             616K  93.2G   144K  /usr
mypool/usr/home                        184K  93.2G   184K  /usr/home
mypool/usr/ports                       144K  93.2G   144K  /usr/ports
mypool/usr/src                         144K  93.2G   144K  /usr/src
mypool/var                            1.29M  93.2G   616K  /var
mypool/var/crash                       148K  93.2G   148K  /var/crash
mypool/var/log                         178K  93.2G   178K  /var/log
mypool/var/mail                        144K  93.2G   144K  /var/mail
mypool/var/newname                    87.5K  93.2G  87.5K  /var/newname
mypool/var/newname@new_snapshot_name      0      -  87.5K  -
mypool/var/tmp                         152K  93.2G   152K  /var/tmp
<span class="c"># zfs snapshot -r mypool@my_recursive_snapshot</span>
<span class="c"># zfs list -t snapshot</span>
NAME                                        USED  AVAIL  REFER  MOUNTPOINT
mypool@my_recursive_snapshot                   0      -   144K  -
mypool/ROOT@my_recursive_snapshot              0      -   144K  -
mypool/ROOT/default@my_recursive_snapshot      0      -   777M  -
mypool/tmp@my_recursive_snapshot               0      -   176K  -
mypool/usr@my_recursive_snapshot               0      -   144K  -
mypool/usr/home@my_recursive_snapshot          0      -   184K  -
mypool/usr/ports@my_recursive_snapshot         0      -   144K  -
mypool/usr/src@my_recursive_snapshot           0      -   144K  -
mypool/var@my_recursive_snapshot               0      -   616K  -
mypool/var/crash@my_recursive_snapshot         0      -   148K  -
mypool/var/log@my_recursive_snapshot           0      -   178K  -
mypool/var/mail@my_recursive_snapshot          0      -   144K  -
mypool/var/newname@new_snapshot_name           0      -  87.5K  -
mypool/var/newname@my_recursive_snapshot       0      -  87.5K  -
mypool/var/tmp@my_recursive_snapshot           0      -   152K  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Snapshots are not shown by a normal <code>zfs list</code> operation. To list snapshots, append <code>-t snapshot</code> to <code>zfs list</code>. <code>-t all</code> displays both file systems and snapshots.</p>
</div>
<div class="paragraph">
<p>Snapshots are not mounted directly, showing no path in the <code>MOUNTPOINT</code> column. ZFS does not mention available disk space in the <code>AVAIL</code> column, as snapshots are read-only after their creation. Compare the snapshot to the original dataset:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -rt all mypool/usr/home</span>
NAME                                    USED  AVAIL  REFER  MOUNTPOINT
mypool/usr/home                         184K  93.2G   184K  /usr/home
mypool/usr/home@my_recursive_snapshot      0      -   184K  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Displaying both the dataset and the snapshot together reveals how snapshots work in <a href="#zfs-term-cow">COW</a> fashion. They save the changes (<em>delta</em>) made and not the complete file system contents all over again. This means that snapshots take little space when making changes. Observe space usage even more by copying a file to the dataset, then creating a second snapshot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp /etc/passwd /var/tmp</span>
<span class="c"># zfs snapshot mypool/var/tmp@after_cp</span>
<span class="c"># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The second snapshot contains the changes to the dataset after the copy operation. This yields enormous space savings. Notice that the size of the snapshot <code><em>mypool/var/tmp@my_recursive_snapshot</em></code> also changed in the <code>USED</code> column to show the changes between itself and the snapshot taken afterwards.</p>
</div>
</div>
<div class="sect3">
<h4 id="zfs-zfs-snapshot-diff">22.4.5.2. Comparing Snapshots<a class="anchor" href="#zfs-zfs-snapshot-diff"></a></h4>
<div class="paragraph">
<p>ZFS provides a built-in command to compare the differences in content between two snapshots. This is helpful with a lot of snapshots taken over time when the user wants to see how the file system has changed over time. For example, <code>zfs diff</code> lets a user find the latest snapshot that still contains a file deleted by accident. Doing this for the two snapshots created in the previous section yields this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         206K  93.2G   118K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp                   0      -   118K  -
<span class="c"># zfs diff mypool/var/tmp@my_recursive_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre>
</div>
</div>
<div class="paragraph">
<p>The command lists the changes between the specified snapshot (in this case <code><em>mypool/var/tmp@my_recursive_snapshot</em></code>) and the live file system. The first column shows the change type:</p>
</div>
<table class="tableblock frame-all grid-all stretch informaltable">
<colgroup>
<col style="width: 20%;"/>
<col style="width: 80%;"/>
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Adding the path or file.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Deleting the path or file.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">M</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Modifying the path or file.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">R</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Renaming the path or file.</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Comparing the output with the table, it becomes clear that ZFS added <span class="filename">passwd</span> after creating the snapshot <code><em>mypool/var/tmp@my_recursive_snapshot</em></code>. This also resulted in a modification to the parent directory mounted at <code><em>/var/tmp</em></code>.</p>
</div>
<div class="paragraph">
<p>Comparing two snapshots is helpful when using the ZFS replication feature to transfer a dataset to a different host for backup purposes.</p>
</div>
<div class="paragraph">
<p>Compare two snapshots by providing the full dataset name and snapshot name of both datasets:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp /var/tmp/passwd /var/tmp/passwd.copy</span>
<span class="c"># zfs snapshot mypool/var/tmp@diff_snapshot</span>
<span class="c"># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@diff_snapshot</span>
M       /var/tmp/
+       /var/tmp/passwd
+       /var/tmp/passwd.copy
<span class="c"># zfs diff mypool/var/tmp@my_recursive_snapshot mypool/var/tmp@after_cp</span>
M       /var/tmp/
+       /var/tmp/passwd</code></pre>
</div>
</div>
<div class="paragraph">
<p>A backup administrator can compare two snapshots received from the sending host and determine the actual changes in the dataset. See the <a href="#zfs-zfs-send">Replication</a> section for more information.</p>
</div>
</div>
<div class="sect3">
<h4 id="zfs-zfs-snapshot-rollback">22.4.5.3. Snapshot Rollback<a class="anchor" href="#zfs-zfs-snapshot-rollback"></a></h4>
<div class="paragraph">
<p>When at least one snapshot is available, roll back to it at any time. Most often this is the case when the current state of the dataset is no longer and if preferring an older version. Scenarios such as local development tests gone wrong, botched system updates hampering the system functionality, or the need to restore deleted files or directories are all too common occurrences. To roll back a snapshot, use <code>zfs rollback <em>snapshotname</em></code>. If a lot of changes are present, the operation will take a long time. During that time, the dataset always remains in a consistent state, much like a database that conforms to ACID principles is performing a rollback. This is happening while the dataset is live and accessible without requiring a downtime. Once the snapshot rolled back, the dataset has the same state as it had when the snapshot was originally taken. Rolling back to a snapshot discards all other data in that dataset not part of the snapshot. Taking a snapshot of the current state of the dataset before rolling back to a previous one is a good idea when requiring some data later. This way, the user can roll back and forth between snapshots without losing data that is still valuable.</p>
</div>
<div class="paragraph">
<p>In the first example, roll back a snapshot because of a careless <code>rm</code> operation that removes too much data than intended.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -rt all mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp                         262K  93.2G   120K  /var/tmp
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class="c"># ls /var/tmp</span>
passwd          passwd.copy     vi.recover
<span class="c"># rm /var/tmp/passwd*</span>
<span class="c"># ls /var/tmp</span>
vi.recover</code></pre>
</div>
</div>
<div class="paragraph">
<p>At this point, the user notices the removal of extra files and wants them back. ZFS provides an easy way to get them back using rollbacks, when performing snapshots of important data on a regular basis. To get the files back and start over from the last snapshot, issue the command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs rollback mypool/var/tmp@diff_snapshot</span>
<span class="c"># ls /var/tmp</span>
passwd          passwd.copy     vi.recover</code></pre>
</div>
</div>
<div class="paragraph">
<p>The rollback operation restored the dataset to the state of the last snapshot. Rolling back to a snapshot taken much earlier with other snapshots taken afterwards is also possible. When trying to do this, ZFS will issue this warning:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -rt snapshot mypool/var/tmp</span>
AME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot    88K      -   152K  -
mypool/var/tmp@after_cp               53.5K      -   118K  -
mypool/var/tmp@diff_snapshot              0      -   120K  -
<span class="c"># zfs rollback mypool/var/tmp@my_recursive_snapshot</span>
cannot rollback to <span class="s1">&#39;mypool/var/tmp@my_recursive_snapshot&#39;</span>: more recent snapshots exist
use <span class="s1">&#39;-r&#39;</span> to force deletion of the following snapshots:
mypool/var/tmp@after_cp
mypool/var/tmp@diff_snapshot</code></pre>
</div>
</div>
<div class="paragraph">
<p>This warning means that snapshots exist between the current state of the dataset and the snapshot to which the user wants to roll back. To complete the rollback delete these snapshots. ZFS cannot track all the changes between different states of the dataset, because snapshots are read-only. ZFS will not delete the affected snapshots unless the user specifies <code>-r</code> to confirm that this is the desired action. If that is the intention, and understanding the consequences of losing all intermediate snapshots, issue the command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs rollback -r mypool/var/tmp@my_recursive_snapshot</span>
<span class="c"># zfs list -rt snapshot mypool/var/tmp</span>
NAME                                   USED  AVAIL  REFER  MOUNTPOINT
mypool/var/tmp@my_recursive_snapshot     8K      -   152K  -
<span class="c"># ls /var/tmp</span>
vi.recover</code></pre>
</div>
</div>
<div class="paragraph">
<p>The output from <code>zfs list -t snapshot</code> confirms the removal of the intermediate snapshots as a result of <code>zfs rollback -r</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="zfs-zfs-snapshot-snapdir">22.4.5.4. Restoring Individual Files from Snapshots<a class="anchor" href="#zfs-zfs-snapshot-snapdir"></a></h4>
<div class="paragraph">
<p>Snapshots live in a hidden directory under the parent dataset: <span class="filename">.zfs/snapshots/snapshotname</span>. By default, these directories will not show even when executing a standard <code>ls -a</code> . Although the directory doesn’t show, access it like any normal directory. The property named <code>snapdir</code> controls whether these hidden directories show up in a directory listing. Setting the property to <code>visible</code> allows them to appear in the output of <code>ls</code> and other commands that deal with directory contents.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get snapdir mypool/var/tmp</span>
NAME            PROPERTY  VALUE    SOURCE
mypool/var/tmp  snapdir   hidden   default
<span class="c"># ls -a /var/tmp</span>
.               ..              passwd          vi.recover
<span class="c"># zfs set snapdir=visible mypool/var/tmp</span>
<span class="c"># ls -a /var/tmp</span>
.               ..              .zfs            passwd          vi.recover</code></pre>
</div>
</div>
<div class="paragraph">
<p>Restore individual files to a previous state by copying them from the snapshot back to the parent dataset. The directory structure below <span class="filename">.zfs/snapshot</span> has a directory named like the snapshots taken earlier to make it easier to identify them. The next example shows how to restore a file from the hidden <span class="filename">.zfs</span> directory by copying it from the snapshot containing the latest version of the file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># rm /var/tmp/passwd</span>
<span class="c"># ls -a /var/tmp</span>
.               ..              .zfs            vi.recover
<span class="c"># ls /var/tmp/.zfs/snapshot</span>
after_cp                my_recursive_snapshot
<span class="c"># ls /var/tmp/.zfs/snapshot/after_cp</span>
passwd          vi.recover
<span class="c"># cp /var/tmp/.zfs/snapshot/after_cp/passwd /var/tmp</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Even if the <code>snapdir</code> property is set to hidden, running <code>ls .zfs/snapshot</code> will still list the contents of that directory. The administrator decides whether to display these directories. This is a per-dataset setting. Copying files or directories from this hidden <span class="filename">.zfs/snapshot</span> is simple enough. Trying it the other way around results in this error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp /etc/rc.conf /var/tmp/.zfs/snapshot/after_cp/</span>
cp: /var/tmp/.zfs/snapshot/after_cp/rc.conf: Read-only file system</code></pre>
</div>
</div>
<div class="paragraph">
<p>The error reminds the user that snapshots are read-only and cannot change after creation. Copying files into and removing them from snapshot directories are both disallowed because that would change the state of the dataset they represent.</p>
</div>
<div class="paragraph">
<p>Snapshots consume space based on how much the parent file system has changed since the time of the snapshot. The <code>written</code> property of a snapshot tracks the space the snapshot uses.</p>
</div>
<div class="paragraph">
<p>To destroy snapshots and reclaim the space, use <code>zfs destroy <em>dataset</em>@<em>snapshot</em></code>. Adding <code>-r</code> recursively removes all snapshots with the same name under the parent dataset. Adding <code>-n -v</code> to the command displays a list of the snapshots to be deleted and an estimate of the space it would reclaim without performing the actual destroy operation.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-clones">22.4.6. Managing Clones<a class="anchor" href="#zfs-zfs-clones"></a></h3>
<div class="paragraph">
<p>A clone is a copy of a snapshot treated more like a regular dataset. Unlike a snapshot, a clone is writeable and mountable, and has its own properties. After creating a clone using <code>zfs clone</code>, destroying the originating snapshot is impossible. To reverse the child/parent relationship between the clone and the snapshot use <code>zfs promote</code>. Promoting a clone makes the snapshot become a child of the clone, rather than of the original parent dataset. This will change how ZFS accounts for the space, but not actually change the amount of space consumed. Mounting the clone anywhere within the ZFS file system hierarchy is possible, not only below the original location of the snapshot.</p>
</div>
<div class="paragraph">
<p>To show the clone feature use this example dataset:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs list -rt all camino/home/joe</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
camino/home/joe         108K   1.3G    87K  /usr/home/joe
camino/home/joe@plans    21K      -  85.5K  -
camino/home/joe@backup    0K      -    87K  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>A typical use for clones is to experiment with a specific dataset while keeping the snapshot around to fall back to in case something goes wrong. Since snapshots cannot change, create a read/write clone of a snapshot. After achieving the desired result in the clone, promote the clone to a dataset and remove the old file system. Removing the parent dataset is not strictly necessary, as the clone and dataset can coexist without problems.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs clone camino/home/joe@backup camino/home/joenew</span>
<span class="c"># ls /usr/home/joe*</span>
/usr/home/joe:
backup.txz     plans.txt

/usr/home/joenew:
backup.txz     plans.txt
<span class="c"># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G     31k    1.3G     0%    /usr/home/joe
usr/home/joenew     1.3G     31k    1.3G     0%    /usr/home/joenew</code></pre>
</div>
</div>
<div class="paragraph">
<p>Creating a clone makes it an exact copy of the state the dataset was in when taking the snapshot. Changing the clone independently from its originating dataset is possible now. The connection between the two is the snapshot. ZFS records this connection in the property <code>origin</code>. Promoting the clone with <code>zfs promote</code> makes the clone an independent dataset. This removes the value of the <code>origin</code> property and disconnects the newly independent dataset from the snapshot. This example shows it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE                     SOURCE
camino/home/joenew    origin    camino/home/joe@backup    -
<span class="c"># zfs promote camino/home/joenew</span>
<span class="c"># zfs get origin camino/home/joenew</span>
NAME                  PROPERTY  VALUE   SOURCE
camino/home/joenew    origin    -       -</code></pre>
</div>
</div>
<div class="paragraph">
<p>After making some changes like copying <span class="filename">loader.conf</span> to the promoted clone, for example, the old directory becomes obsolete in this case. Instead, the promoted clone can replace it. To do this, <code>zfs destroy</code> the old dataset first and then <code>zfs rename</code> the clone to the old dataset name (or to an entirely different name).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># cp /boot/defaults/loader.conf /usr/home/joenew</span>
<span class="c"># zfs destroy -f camino/home/joe</span>
<span class="c"># zfs rename camino/home/joenew camino/home/joe</span>
<span class="c"># ls /usr/home/joe</span>
backup.txz     loader.conf     plans.txt
<span class="c"># df -h /usr/home</span>
Filesystem          Size    Used   Avail Capacity  Mounted on
usr/home/joe        1.3G    128k    1.3G     0%    /usr/home/joe</code></pre>
</div>
</div>
<div class="paragraph">
<p>The cloned snapshot is now an ordinary dataset. It contains all the data from the original snapshot plus the files added to it like <span class="filename">loader.conf</span>. Clones provide useful features to ZFS users in different scenarios. For example, provide jails as snapshots containing different sets of installed applications. Users can clone these snapshots and add their own applications as they see fit. Once satisfied with the changes, promote the clones to full datasets and provide them to end users to work with like they would with a real dataset. This saves time and administrative overhead when providing these jails.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-send">22.4.7. Replication<a class="anchor" href="#zfs-zfs-send"></a></h3>
<div class="paragraph">
<p>Keeping data on a single pool in one location exposes it to risks like theft and natural or human disasters. Making regular backups of the entire pool is vital. ZFS provides a built-in serialization feature that can send a stream representation of the data to standard output. Using this feature, storing this data on another pool connected to the local system is possible, as is sending it over a network to another system. Snapshots are the basis for this replication (see the section on <a href="#zfs-zfs-snapshot">ZFS snapshots</a>). The commands used for replicating data are <code>zfs send</code> and <code>zfs receive</code>.</p>
</div>
<div class="paragraph">
<p>These examples show ZFS replication with these two pools:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M    77K   896M         -         -     0%    0%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%    4%  1.00x  ONLINE  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The pool named <em>mypool</em> is the primary pool where writing and reading data happens on a regular basis. Using a second standby pool <em>backup</em> in case the primary pool becomes unavailable. Note that this fail-over is not done automatically by ZFS, but must be manually done by a system administrator when needed. Use a snapshot to provide a consistent file system version to replicate. After creating a snapshot of <em>mypool</em>, copy it to the <em>backup</em> pool by replicating snapshots. This does not include changes made since the most recent snapshot.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs snapshot mypool@backup1</span>
<span class="c"># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@backup1             0      -  43.6M  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now that a snapshot exists, use <code>zfs send</code> to create a stream representing the contents of the snapshot. Store this stream as a file or receive it on another pool. Write the stream to standard output, but redirect to a file or pipe or an error appears:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs send mypool@backup1</span>
Error: Stream can not be written to a terminal.
You must redirect standard output.</code></pre>
</div>
</div>
<div class="paragraph">
<p>To back up a dataset with <code>zfs send</code>, redirect to a file located on the mounted backup pool. Ensure that the pool has enough free space to accommodate the size of the sent snapshot, which means the data contained in the snapshot, not the changes from the previous snapshot.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs send mypool@backup1 &gt; /backup/backup1</span>
<span class="c"># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>zfs send</code> transferred all the data in the snapshot called <em>backup1</em> to the pool named <em>backup</em>. To create and send these snapshots automatically, use a <a href="https://man.freebsd.org/cgi/man.cgi?query=cron&amp;sektion=8&amp;format=html">cron(8)</a> job.</p>
</div>
<div class="paragraph">
<p>Instead of storing the backups as archive files, ZFS can receive them as a live file system, allowing direct access to the backed up data. To get to the actual data contained in those streams, use <code>zfs receive</code> to transform the streams back into files and directories. The example below combines <code>zfs send</code> and <code>zfs receive</code> using a pipe to copy the data from one pool to another. Use the data directly on the receiving pool after the transfer is complete. It is only possible to replicate a dataset to an empty dataset.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs snapshot mypool@replica1</span>
<span class="c"># zfs send -v mypool@replica1 | zfs receive backup/mypool</span>
send from @ to mypool@replica1 estimated size is 50.1M
total estimated size is 50.1M
TIME        SENT   SNAPSHOT

<span class="c"># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
backup  960M  63.7M   896M         -         -     0%     6%  1.00x  ONLINE  -
mypool  984M  43.7M   940M         -         -     0%     4%  1.00x  ONLINE  -</code></pre>
</div>
</div>
<div class="sect3">
<h4 id="zfs-send-incremental">22.4.7.1. Incremental Backups<a class="anchor" href="#zfs-send-incremental"></a></h4>
<div class="paragraph">
<p><code>zfs send</code> can also determine the difference between two snapshots and send individual differences between the two. This saves disk space and transfer time. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs snapshot mypool@replica2</span>
<span class="c"># zfs list -t snapshot</span>
NAME                    USED  AVAIL  REFER  MOUNTPOINT
mypool@replica1         5.72M      -  43.6M  -
mypool@replica2             0      -  44.1M  -
<span class="c"># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
backup  960M  61.7M   898M         -         -     0%    6%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%    5%  1.00x  ONLINE  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a second snapshot called <em>replica2</em>. This second snapshot contains changes made to the file system between now and the previous snapshot, <em>replica1</em>. Using <code>zfs send -i</code> and indicating the pair of snapshots generates an incremental replica stream containing the changed data. This succeeds if the initial snapshot already exists on the receiving side.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs send -v -i mypool@replica1 mypool@replica2 | zfs receive /backup/mypool</span>
send from @replica1 to mypool@replica2 estimated size is 5.02M
total estimated size is 5.02M
TIME        SENT   SNAPSHOT

<span class="c"># zpool list</span>
NAME    SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG  CAP  DEDUP  HEALTH  ALTROOT
backup  960M  80.8M   879M         -         -     0%   8%  1.00x  ONLINE  -
mypool  960M  50.2M   910M         -         -     0%   5%  1.00x  ONLINE  -

<span class="c"># zfs list</span>
NAME                         USED  AVAIL  REFER  MOUNTPOINT
backup                      55.4M   240G   152K  /backup
backup/mypool               55.3M   240G  55.2M  /backup/mypool
mypool                      55.6M  11.6G  55.0M  /mypool

<span class="c"># zfs list -t snapshot</span>
NAME                                         USED  AVAIL  REFER  MOUNTPOINT
backup/mypool@replica1                       104K      -  50.2M  -
backup/mypool@replica2                          0      -  55.2M  -
mypool@replica1                             29.9K      -  50.0M  -
mypool@replica2                                 0      -  55.0M  -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The incremental stream replicated the changed data rather than the entirety of <em>replica1</em>. Sending the differences alone took much less time to transfer and saved disk space by not copying the whole pool each time. This is useful when replicating over a slow network or one charging per transferred byte.</p>
</div>
<div class="paragraph">
<p>A new file system, <em>backup/mypool</em>, is available with the files and data from the pool <em>mypool</em>. Specifying <code>-p</code> copies the dataset properties including compression settings, quotas, and mount points. Specifying <code>-R</code> copies all child datasets of the dataset along with their properties. Automate sending and receiving to create regular backups on the second pool.</p>
</div>
</div>
<div class="sect3">
<h4 id="zfs-send-ssh">22.4.7.2. Sending Encrypted Backups over SSH<a class="anchor" href="#zfs-send-ssh"></a></h4>
<div class="paragraph">
<p>Sending streams over the network is a good way to keep a remote backup, but it does come with a drawback. Data sent over the network link is not encrypted, allowing anyone to intercept and transform the streams back into data without the knowledge of the sending user. This is undesirable when sending the streams over the internet to a remote host. Use SSH to securely encrypt data sent over a network connection. Since ZFS requires redirecting the stream from standard output, piping it through SSH is easy. To keep the contents of the file system encrypted in transit and on the remote system, consider using <a href="https://wiki.freebsd.org/PEFS">PEFS</a>.</p>
</div>
<div class="paragraph">
<p>Change some settings and take security precautions first. This describes the necessary steps required for the <code>zfs send</code> operation; for more information on SSH, see <a href="../security/#openssh">OpenSSH</a>.</p>
</div>
<div class="paragraph">
<p>Change the configuration as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Passwordless SSH access between sending and receiving host using SSH keys</p>
</li>
<li>
<p>ZFS requires the privileges of the <code>root</code> user to send and receive streams. This requires logging in to the receiving system as <code>root</code>.</p>
</li>
<li>
<p>Security reasons prevent <code>root</code> from logging in by default.</p>
</li>
<li>
<p>Use the <a href="#zfs-zfs-allow">ZFS Delegation</a> system to allow a non-<code>root</code> user on each system to perform the respective send and receive operations. On the sending system:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs allow -u someuser send,snapshot mypool</span></code></pre>
</div>
</div>
</li>
<li>
<p>To mount the pool, the unprivileged user must own the directory, and regular users need permission to mount file systems.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>On the receiving system:</p>
</div>
<div class="paragraph">
<p>+</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># sysctl vfs.usermount=1</span>
vfs.usermount: 0 -&gt; 1
<span class="c"># echo vfs.usermount=1 &gt;&gt; /etc/sysctl.conf</span>
<span class="c"># zfs create recvpool/backup</span>
<span class="c"># zfs allow -u someuser create,mount,receive recvpool/backup</span>
<span class="c"># chown someuser /recvpool/backup</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The unprivileged user can receive and mount datasets now, and replicates the <em>home</em> dataset to the remote system:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="gp">% </span>zfs snapshot -r mypool/home@monday
<span class="gp">% </span>zfs send -R mypool/home@monday | ssh someuser@backuphost zfs recv -dvu recvpool/backup</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a recursive snapshot called <em>monday</em> of the file system dataset <em>home</em> on the pool <em>mypool</em>. Then <code>zfs send -R</code> includes the dataset, all child datasets, snapshots, clones, and settings in the stream. Pipe the output through SSH to the waiting <code>zfs receive</code> on the remote host <em>backuphost</em>. Using an IP address or fully qualified domain name is good practice. The receiving machine writes the data to the <em>backup</em> dataset on the <em>recvpool</em> pool. Adding <code>-d</code> to <code>zfs recv</code> overwrites the name of the pool on the receiving side with the name of the snapshot. <code>-u</code> causes the file systems to not mount on the receiving side. Using <code>-v</code> shows more details about the transfer, including the elapsed time and the amount of data transferred.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-quota">22.4.8. Dataset, User, and Group Quotas<a class="anchor" href="#zfs-zfs-quota"></a></h3>
<div class="paragraph">
<p>Use <a href="#zfs-term-quota">Dataset quotas</a> to restrict the amount of space consumed by a particular dataset. <a href="#zfs-term-refquota">Reference Quotas</a> work in much the same way, but count the space used by the dataset itself, excluding snapshots and child datasets. Similarly, use <a href="#zfs-term-userquota">user</a> and <a href="#zfs-term-groupquota">group</a> quotas to prevent users or groups from using up all the space in the pool or dataset.</p>
</div>
<div class="paragraph">
<p>The following examples assume that the users already exist in the system. Before adding a user to the system, make sure to create their home dataset first and set the <code>mountpoint</code> to <code>/home/<em>bob</em></code>. Then, create the user and make the home directory point to the dataset’s <code>mountpoint</code> location. This will properly set owner and group permissions without shadowing any pre-existing home directory paths that might exist.</p>
</div>
<div class="paragraph">
<p>To enforce a dataset quota of 10 GB for <span class="filename">storage/home/bob</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set quota=10G storage/home/bob</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To enforce a reference quota of 10 GB for <span class="filename">storage/home/bob</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set refquota=10G storage/home/bob</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove a quota of 10 GB for <span class="filename">storage/home/bob</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set quota=none storage/home/bob</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The general format is <code>userquota@<em>user</em>=<em>size</em></code>, and the user’s name must be in one of these formats:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>POSIX compatible name such as <em>joe</em>.</p>
</li>
<li>
<p>POSIX numeric ID such as <em>789</em>.</p>
</li>
<li>
<p>SID name such as <em>joe.bloggs@example.com</em>.</p>
</li>
<li>
<p>SID numeric ID such as <em>S-1-123-456-789</em>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, to enforce a user quota of 50 GB for the user named <em>joe</em>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set userquota@joe=50G</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove any quota:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set userquota@joe=none</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>User quota properties are not displayed by <code>zfs get all</code>. Non-<code>root</code> users can’t see other’s quotas unless granted the <code>userquota</code> privilege. Users with this privilege are able to view and set everyone’s quota.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
<div class="paragraph">
<p>The general format for setting a group quota is: <code>groupquota@<em>group</em>=<em>size</em></code>.</p>
</div>
<div class="paragraph">
<p>To set the quota for the group <em>firstgroup</em> to 50 GB, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set groupquota@firstgroup=50G</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To remove the quota for the group <em>firstgroup</em>, or to make sure that one is not set, instead use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set groupquota@firstgroup=none</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>As with the user quota property, non-<code>root</code> users can see the quotas associated with the groups to which they belong. A user with the <code>groupquota</code> privilege or <code>root</code> can view and set all quotas for all groups.</p>
</div>
<div class="paragraph">
<p>To display the amount of space used by each user on a file system or snapshot along with any quotas, use <code>zfs userspace</code>. For group information, use <code>zfs groupspace</code>. For more information about supported options or how to display specific options alone, refer to <a href="https://man.freebsd.org/cgi/man.cgi?query=zfs&amp;sektion=1&amp;format=html">zfs(1)</a>.</p>
</div>
<div class="paragraph">
<p>Privileged users and <code>root</code> can list the quota for <span class="filename">storage/home/bob</span> using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get quota storage/home/bob</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-reservation">22.4.9. Reservations<a class="anchor" href="#zfs-zfs-reservation"></a></h3>
<div class="paragraph">
<p><a href="#zfs-term-reservation">Reservations</a> guarantee an always-available amount of space on a dataset. The reserved space will not be available to any other dataset. This useful feature ensures that free space is available for an important dataset or log files.</p>
</div>
<div class="paragraph">
<p>The general format of the <code>reservation</code> property is <code>reservation=<em>size</em></code>, so to set a reservation of 10 GB on <span class="filename">storage/home/bob</span>, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set reservation=10G storage/home/bob</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To clear any reservation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set reservation=none storage/home/bob</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>The same principle applies to the <code>refreservation</code> property for setting a <a href="#zfs-term-refreservation">Reference Reservation</a>, with the general format <code>refreservation=<em>size</em></code>.</p>
</div>
<div class="paragraph">
<p>This command shows any reservations or refreservations that exist on <span class="filename">storage/home/bob</span>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get reservation storage/home/bob</span>
<span class="c"># zfs get refreservation storage/home/bob</span></code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-compression">22.4.10. Compression<a class="anchor" href="#zfs-zfs-compression"></a></h3>
<div class="paragraph">
<p>ZFS provides transparent compression. Compressing data written at the block level saves space and also increases disk throughput. If data compresses by 25% the compressed data writes to the disk at the same rate as the uncompressed version, resulting in an effective write speed of 125%. Compression can also be a great alternative to <a href="#zfs-zfs-deduplication">Deduplication</a> because it does not require extra memory.</p>
</div>
<div class="paragraph">
<p>ZFS offers different compression algorithms, each with different trade-offs. The introduction of LZ4 compression in ZFS v5000 enables compressing the entire pool without the large performance trade-off of other algorithms. The biggest advantage to LZ4 is the <em>early abort</em> feature. If LZ4 does not achieve at least 12.5% compression in the header part of the data, ZFS writes the block uncompressed to avoid wasting CPU cycles trying to compress data that is either already compressed or uncompressible. For details about the different compression algorithms available in ZFS, see the <a href="#zfs-term-compression">Compression</a> entry in the terminology section.</p>
</div>
<div class="paragraph">
<p>The administrator can see the effectiveness of compression using dataset properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs get used,compressratio,compression,logicalused mypool/compressed_dataset</span>
NAME        PROPERTY          VALUE     SOURCE
mypool/compressed_dataset  used              449G      -
mypool/compressed_dataset  compressratio     1.11x     -
mypool/compressed_dataset  compression       lz4       <span class="nb">local
</span>mypool/compressed_dataset  logicalused       496G      -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The dataset is using 449 GB of space (the used property). Without compression, it would have taken 496 GB of space (the <code>logicalused</code> property). This results in a 1.11:1 compression ratio.</p>
</div>
<div class="paragraph">
<p>Compression can have an unexpected side effect when combined with <a href="#zfs-term-userquota">User Quotas</a>. User quotas restrict how much actual space a user consumes on a dataset <em>after compression</em>. If a user has a quota of 10 GB, and writes 10 GB of compressible data, they will still be able to store more data. If they later update a file, say a database, with more or less compressible data, the amount of space available to them will change. This can result in the odd situation where a user did not increase the actual amount of data (the <code>logicalused</code> property), but the change in compression caused them to reach their quota limit.</p>
</div>
<div class="paragraph">
<p>Compression can have a similar unexpected interaction with backups. Quotas are often used to limit data storage to ensure there is enough backup space available. Since quotas do not consider compression ZFS may write more data than would fit with uncompressed backups.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-compression-zstd">22.4.11. Zstandard Compression<a class="anchor" href="#zfs-zfs-compression-zstd"></a></h3>
<div class="paragraph">
<p>OpenZFS 2.0 added a new compression algorithm. Zstandard (Zstd) offers higher compression ratios than the default LZ4 while offering much greater speeds than the alternative, gzip. OpenZFS 2.0 is available starting with FreeBSD 12.1-RELEASE via <a class="package" href="https://cgit.freebsd.org/ports/tree/sysutils/openzfs/">sysutils/openzfs</a> and has been the default in since FreeBSD 13.0-RELEASE.</p>
</div>
<div class="paragraph">
<p>Zstd provides a large selection of compression levels, providing fine-grained control over performance versus compression ratio. One of the main advantages of Zstd is that the decompression speed is independent of the compression level. For data written once but read often, Zstd allows the use of the highest compression levels without a read performance penalty.</p>
</div>
<div class="paragraph">
<p>Even with frequent data updates, enabling compression often provides higher performance. One of the biggest advantages comes from the compressed ARC feature. ZFS’s Adaptive Replacement Cache (ARC) caches the compressed version of the data in RAM, decompressing it each time. This allows the same amount of RAM to store more data and metadata, increasing the cache hit ratio.</p>
</div>
<div class="paragraph">
<p>ZFS offers 19 levels of Zstd compression, each offering incrementally more space savings in exchange for slower compression. The default level is <code>zstd-3</code> and offers greater compression than LZ4 without being much slower. Levels above 10 require large amounts of memory to compress each block and systems with less than 16 GB of RAM should not use them. ZFS uses a selection of the Zstd_fast_ levels also, which get correspondingly faster but supports lower compression ratios. ZFS supports <code>zstd-fast-1</code> through <code>zstd-fast-10</code>, <code>zstd-fast-20</code> through <code>zstd-fast-100</code> in increments of 10, and <code>zstd-fast-500</code> and <code>zstd-fast-1000</code> which provide minimal compression, but offer high performance.</p>
</div>
<div class="paragraph">
<p>If ZFS is not able to get the required memory to compress a block with Zstd, it will fall back to storing the block uncompressed. This is unlikely to happen except at the highest levels of Zstd on memory constrained systems. ZFS counts how often this has occurred since loading the ZFS module with <code>kstat.zfs.misc.zstd.compress_alloc_fail</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-deduplication">22.4.12. Deduplication<a class="anchor" href="#zfs-zfs-deduplication"></a></h3>
<div class="paragraph">
<p>When enabled, <a href="#zfs-term-deduplication">deduplication</a> uses the checksum of each block to detect duplicate blocks. When a new block is a duplicate of an existing block, ZFS writes a new reference to the existing data instead of the whole duplicate block. Tremendous space savings are possible if the data contains a lot of duplicated files or repeated information. Warning: deduplication requires a large amount of memory, and enabling compression instead provides most of the space savings without the extra cost.</p>
</div>
<div class="paragraph">
<p>To activate deduplication, set the <code>dedup</code> property on the target pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zfs set dedup=on pool</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Deduplicating only affects new data written to the pool. Merely activating this option will not deduplicate data already written to the pool. A pool with a freshly activated deduplication property will look like this example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool list</span>
NAME  SIZE ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG   CAP   DEDUP   HEALTH   ALTROOT
pool 2.84G 2.19M 2.83G         -         -     0%    0%   1.00x   ONLINE   -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>DEDUP</code> column shows the actual rate of deduplication for the pool. A value of <code>1.00x</code> shows that data has not deduplicated yet. The next example copies some system binaries three times into different directories on the deduplicated pool created above.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># for d in dir1 dir2 dir3; do</span>
<span class="gp">&gt; </span>mkdir <span class="nv">$d</span> <span class="o">&amp;&amp;</span> cp -R /usr/bin <span class="nv">$d</span> &amp;
<span class="gp">&gt; </span><span class="k">done</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>To observe deduplicating of redundant data, use:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zpool list</span>
NAME SIZE  ALLOC  FREE   CKPOINT  EXPANDSZ   FRAG  CAP   DEDUP   HEALTH   ALTROOT
pool 2.84G 20.9M 2.82G         -         -     0%   0%   3.00x   ONLINE   -</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>DEDUP</code> column shows a factor of <code>3.00x</code>. Detecting and deduplicating copies of the data uses a third of the space. The potential for space savings can be enormous, but comes at the cost of having enough memory to keep track of the deduplicated blocks.</p>
</div>
<div class="paragraph">
<p>Deduplication is not always beneficial when the data in a pool is not redundant. ZFS can show potential space savings by simulating deduplication on an existing pool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="shell"><span class="c"># zdb -S pool</span>
Simulated DDT histogram:

bucket              allocated                       referenced
______   ______________________________   ______________________________
refcnt   blocks   LSIZE   PSIZE   DSIZE   blocks   LSIZE   PSIZE   DSIZE
------   ------   -----   -----   -----   ------   -----   -----   -----
     1    2.58M    289G    264G    264G    2.58M    289G    264G    264G
     2     206K   12.6G   10.4G   10.4G     430K   26.4G   21.6G   21.6G
     4    37.6K    692M    276M    276M     170K   3.04G   1.26G   1.26G
     8    2.18K   45.2M   19.4M   19.4M    20.0K    425M    176M    176M
    16      174   2.83M   1.20M   1.20M    3.33K   48.4M   20.4M   20.4M
    32       40   2.17M    222K    222K    1.70K   97.2M   9.91M   9.91M
    64        9     56K   10.5K   10.5K      865   4.96M    948K    948K
   128        2   9.50K      2K      2K      419   2.11M    438K    438K
   256        5   61.5K     12K     12K    1.90K   23.0M   4.47M   4.47M
    1K        2      1K      1K      1K    2.98K   1.49M   1.49M   1.49M
 Total    2.82M    303G    275G    275G    3.20M    319G    287G    287G

dedup <span class="o">=</span> 1.05, compress <span class="o">=</span> 1.11, copies <span class="o">=</span> 1.00, dedup <span class="k">*</span> compress / copies <span class="o">=</span> 1.16</code></pre>
</div>
</div>
<div class="paragraph">
<p>After <code>zdb -S</code> finishes analyzing the pool, it shows the space reduction ratio that activating deduplication would achieve. In this case, <code>1.16</code> is a poor space saving ratio mainly provided by compression. Activating deduplication on this pool would not save any amount of space, and is not worth the amount of memory required to enable deduplication. Using the formula <em>ratio = dedup * compress / copies</em>, system administrators can plan the storage allocation, deciding whether the workload will contain enough duplicate blocks to justify the memory requirements. If the data is reasonably compressible, the space savings may be good. Good practice is to enable compression first as compression also provides greatly increased performance. Enable deduplication in cases where savings are considerable and with enough available memory for the <a href="#zfs-term-deduplication">DDT</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-jail">22.4.13. ZFS and Jails<a class="anchor" href="#zfs-zfs-jail"></a></h3>
<div class="paragraph">
<p>Use <code>zfs jail</code> and the corresponding <code>jailed</code> property to delegate a ZFS dataset to a <a href="../jails/#jails">Jail</a>. <code>zfs jail <em>jailid</em></code> attaches a dataset to the specified jail, and <code>zfs unjail</code> detaches it. To control the dataset from within a jail, set the <code>jailed</code> property. ZFS forbids mounting a jailed dataset on the host because it may have mount points that would compromise the security of the host.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-zfs-allow">22.5. Delegated Administration<a class="anchor" href="#zfs-zfs-allow"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>A comprehensive permission delegation system allows unprivileged users to perform ZFS administration functions. For example, if each user’s home directory is a dataset, users need permission to create and destroy snapshots of their home directories. A user performing backups can get permission to use replication features. ZFS allows a usage statistics script to run with access to only the space usage data for all users. Delegating the ability to delegate permissions is also possible. Permission delegation is possible for each subcommand and most properties.</p>
</div>
<div class="sect2">
<h3 id="zfs-zfs-allow-create">22.5.1. Delegating Dataset Creation<a class="anchor" href="#zfs-zfs-allow-create"></a></h3>
<div class="paragraph">
<p><code>zfs allow <em>someuser</em> create <em>mydataset</em></code> gives the specified user permission to create child datasets under the selected parent dataset. A caveat: creating a new dataset involves mounting it. That requires setting the FreeBSD <code>vfs.usermount</code> <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> to <code>1</code> to allow non-root users to mount a file system. Another restriction aimed at preventing abuse: non-<code>root</code> users must own the mountpoint where mounting the file system.</p>
</div>
</div>
<div class="sect2">
<h3 id="zfs-zfs-allow-allow">22.5.2. Delegating Permission Delegation<a class="anchor" href="#zfs-zfs-allow-allow"></a></h3>
<div class="paragraph">
<p><code>zfs allow <em>someuser</em> allow <em>mydataset</em></code> gives the specified user the ability to assign any permission they have on the target dataset, or its children, to other users. If a user has the <code>snapshot</code> permission and the <code>allow</code> permission, that user can then grant the <code>snapshot</code> permission to other users.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-advanced">22.6. Advanced Topics<a class="anchor" href="#zfs-advanced"></a></h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="zfs-advanced-tuning">22.6.1. Tuning<a class="anchor" href="#zfs-advanced-tuning"></a></h3>
<div class="paragraph">
<p>Adjust tunables to make ZFS perform best for different workloads.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a id="zfs-advanced-tuning-arc_max"></a> <code><em>vfs.zfs.arc.max</em></code> starting with 13.x (<code>vfs.zfs.arc_max</code> for 12.x) - Upper size of the <a href="#zfs-term-arc">ARC</a>. The default is all RAM but 1 GB, or 5/8 of all RAM, whichever is more. Use a lower value if the system runs any other daemons or processes that may require memory. Adjust this value at runtime with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> and set it in <span class="filename">/boot/loader.conf</span> or <span class="filename">/etc/sysctl.conf</span>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-arc_meta_limit"></a> <code><em>vfs.zfs.arc.meta_limit</em></code> starting with 13.x (<code>vfs.zfs.arc_meta_limit</code> for 12.x)` - Limit the amount of the <a href="#zfs-term-arc">ARC</a> used to store metadata. The default is one fourth of <code>vfs.zfs.arc.max</code>. Increasing this value will improve performance if the workload involves operations on a large number of files and directories, or frequent metadata operations, at the cost of less file data fitting in the <a href="#zfs-term-arc">ARC</a>. Adjust this value at runtime with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> in <span class="filename">/boot/loader.conf</span> or <span class="filename">/etc/sysctl.conf</span>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-arc_min"></a> <code><em>vfs.zfs.arc.min</em></code> starting with 13.x (<code>vfs.zfs.arc_min</code> for 12.x) - Lower size of the <a href="#zfs-term-arc">ARC</a>. The default is one half of <code>vfs.zfs.arc.meta_limit</code>. Adjust this value to prevent other applications from pressuring out the entire <a href="#zfs-term-arc">ARC</a>. Adjust this value at runtime with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> and in <span class="filename">/boot/loader.conf</span> or <span class="filename">/etc/sysctl.conf</span>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-vdev-cache-size"></a> <code><em>vfs.zfs.vdev.cache.size</em></code> - A preallocated amount of memory reserved as a cache for each device in the pool. The total amount of memory used will be this value multiplied by the number of devices. Set this value at boot time and in <span class="filename">/boot/loader.conf</span>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-min-auto-ashift"></a> <code><em>vfs.zfs.min_auto_ashift</em></code> - Lower <code>ashift</code> (sector size) used automatically at pool creation time. The value is a power of two. The default value of <code>9</code> represents <code>2^9 = 512</code>, a sector size of 512 bytes. To avoid <em>write amplification</em> and get the best performance, set this value to the largest sector size used by a device in the pool.</p>
<div class="paragraph">
<p>Common drives have 4 KB sectors. Using the default <code>ashift</code> of <code>9</code> with these drives results in write amplification on these devices. Data contained in a single 4 KB write is instead written in eight 512-byte writes. ZFS tries to read the native sector size from all devices when creating a pool, but drives with 4 KB sectors report that their sectors are 512 bytes for compatibility. Setting <code>vfs.zfs.min_auto_ashift</code> to <code>12</code> (<code>2^12 = 4096</code>) before creating a pool forces ZFS to use 4 KB blocks for best performance on these drives.</p>
</div>
<div class="paragraph">
<p>Forcing 4 KB blocks is also useful on pools with planned disk upgrades. Future disks use 4 KB sectors, and <code>ashift</code> values cannot change after creating a pool.</p>
</div>
<div class="paragraph">
<p>In some specific cases, the smaller 512-byte block size might be preferable. When used with 512-byte disks for databases or as storage for virtual machines, less data transfers during small random reads. This can provide better performance when using a smaller ZFS record size.</p>
</div>
</li>
<li>
<p><a id="zfs-advanced-tuning-prefetch_disable"></a> <code><em>vfs.zfs.prefetch_disable</em></code> - Disable prefetch. A value of <code>0</code> enables and <code>1</code> disables it. The default is <code>0</code>, unless the system has less than 4 GB of RAM. Prefetch works by reading larger blocks than requested into the <a href="#zfs-term-arc">ARC</a> in hopes to soon need the data. If the workload has a large number of random reads, disabling prefetch may actually improve performance by reducing unnecessary reads. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-vdev-trim_on_init"></a> <code><em>vfs.zfs.vdev.trim_on_init</em></code> - Control whether new devices added to the pool have the <code>TRIM</code> command run on them. This ensures the best performance and longevity for SSDs, but takes extra time. If the device has already been secure erased, disabling this setting will make the addition of the new device faster. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-vdev-max_pending"></a> <code><em>vfs.zfs.vdev.max_pending</em></code> - Limit the number of pending I/O requests per device. A higher value will keep the device command queue full and may give higher throughput. A lower value will reduce latency. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-top_maxinflight"></a> <code><em>vfs.zfs.top_maxinflight</em></code> - Upper number of outstanding I/Os per top-level <a href="#zfs-term-vdev">vdev</a>. Limits the depth of the command queue to prevent high latency. The limit is per top-level vdev, meaning the limit applies to each <a href="#zfs-term-vdev-mirror">mirror</a>, <a href="#zfs-term-vdev-raidz">RAID-Z</a>, or other vdev independently. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-l2arc_write_max"></a> <code><em>vfs.zfs.l2arc_write_max</em></code> - Limit the amount of data written to the <a href="#zfs-term-l2arc">L2ARC</a> per second. This tunable extends the longevity of SSDs by limiting the amount of data written to the device. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-l2arc_write_boost"></a> <code><em>vfs.zfs.l2arc_write_boost</em></code> - Adds the value of this tunable to <a href="#zfs-advanced-tuning-l2arc_write_max"><code>vfs.zfs.l2arc_write_max</code></a> and increases the write speed to the SSD until evicting the first block from the <a href="#zfs-term-l2arc">L2ARC</a>. This &#34;Turbo Warmup Phase&#34; reduces the performance loss from an empty <a href="#zfs-term-l2arc">L2ARC</a> after a reboot. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-scrub_delay"></a><code><em>vfs.zfs.scrub_delay</em></code> - Number of ticks to delay between each I/O during a <a href="#zfs-term-scrub"><code>scrub</code></a>. To ensure that a <code>scrub</code> does not interfere with the normal operation of the pool, if any other I/O is happening the <code>scrub</code> will delay between each command. This value controls the limit on the total IOPS (I/Os Per Second) generated by the <code>scrub</code>. The granularity of the setting is determined by the value of <code>kern.hz</code> which defaults to 1000 ticks per second. Changing this setting results in a different effective IOPS limit. The default value is <code>4</code>, resulting in a limit of: 1000 ticks/sec / 4 = 250 IOPS. Using a value of <em>20</em> would give a limit of: 1000 ticks/sec / 20 = 50 IOPS. Recent activity on the pool limits the speed of <code>scrub</code>, as determined by <a href="#zfs-advanced-tuning-scan_idle"><code>vfs.zfs.scan_idle</code></a>. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-resilver_delay"></a> <code><em>vfs.zfs.resilver_delay</em></code> - Number of milliseconds of delay inserted between each I/O during a <a href="#zfs-term-resilver">resilver</a>. To ensure that a resilver does not interfere with the normal operation of the pool, if any other I/O is happening the resilver will delay between each command. This value controls the limit of total IOPS (I/Os Per Second) generated by the resilver. ZFS determins the granularity of the setting by the value of <code>kern.hz</code> which defaults to 1000 ticks per second. Changing this setting results in a different effective IOPS limit. The default value is 2, resulting in a limit of: 1000 ticks/sec / 2 = 500 IOPS. Returning the pool to an <a href="#zfs-term-online">Online</a> state may be more important if another device failing could <a href="#zfs-term-faulted">Fault</a> the pool, causing data loss. A value of 0 will give the resilver operation the same priority as other operations, speeding the healing process. Other recent activity on the pool limits the speed of resilver, as determined by <a href="#zfs-advanced-tuning-scan_idle"><code>vfs.zfs.scan_idle</code></a>. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-scan_idle"></a> <code><em>vfs.zfs.scan_idle</em></code> - Number of milliseconds since the last operation before considering the pool is idle. ZFS disables the rate limiting for <a href="#zfs-term-scrub"><code>scrub</code></a> and <a href="#zfs-term-resilver">resilver</a> when the pool is idle. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
<li>
<p><a id="zfs-advanced-tuning-txg-timeout"></a> <code><em>vfs.zfs.txg.timeout</em></code> - Upper number of seconds between <a href="#zfs-term-txg">transaction group</a>s. The current transaction group writes to the pool and a fresh transaction group starts if this amount of time elapsed since the previous transaction group. A transaction group may trigger earlier if writing enough data. The default value is 5 seconds. A larger value may improve read performance by delaying asynchronous writes, but this may cause uneven performance when writing the transaction group. Adjust this value at any time with <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="zfs-advanced-i386">22.6.2. ZFS on i386<a class="anchor" href="#zfs-advanced-i386"></a></h3>
<div class="paragraph">
<p>Some of the features provided by ZFS are memory intensive, and may require tuning for upper efficiency on systems with limited RAM.</p>
</div>
<div class="sect3">
<h4 id="_memory">22.6.2.1. Memory<a class="anchor" href="#_memory"></a></h4>
<div class="paragraph">
<p>As a lower value, the total system memory should be at least one gigabyte. The amount of recommended RAM depends upon the size of the pool and which features ZFS uses. A general rule of thumb is 1 GB of RAM for every 1 TB of storage. If using the deduplication feature, a general rule of thumb is 5 GB of RAM per TB of storage to deduplicate. While some users use ZFS with less RAM, systems under heavy load may panic due to memory exhaustion. ZFS may require further tuning for systems with less than the recommended RAM requirements.</p>
</div>
</div>
<div class="sect3">
<h4 id="_kernel_configuration">22.6.2.2. Kernel Configuration<a class="anchor" href="#_kernel_configuration"></a></h4>
<div class="paragraph">
<p>Due to the address space limitations of the i386™ platform, ZFS users on the i386™ architecture must add this option to a custom kernel configuration file, rebuild the kernel, and reboot:</p>
</div>
<div class="literalblock programlisting">
<div class="content">
<pre>options        KVA_PAGES=512</pre>
</div>
</div>
<div class="paragraph">
<p>This expands the kernel address space, allowing the <code>vm.kvm_size</code> tunable to push beyond the imposed limit of 1 GB, or the limit of 2 GB for PAE. To find the most suitable value for this option, divide the desired address space in megabytes by four. In this example <code>512</code> for 2 GB.</p>
</div>
</div>
<div class="sect3">
<h4 id="_loader_tunables">22.6.2.3. Loader Tunables<a class="anchor" href="#_loader_tunables"></a></h4>
<div class="paragraph">
<p>Increases the <span class="filename">kmem</span> address space on all FreeBSD architectures. A test system with 1 GB of physical memory benefitted from adding these options to <span class="filename">/boot/loader.conf</span> and then restarting:</p>
</div>
<div class="literalblock programlisting">
<div class="content">
<pre>vm.kmem_size=&#34;330M&#34;
vm.kmem_size_max=&#34;330M&#34;
vfs.zfs.arc.max=&#34;40M&#34;
vfs.zfs.vdev.cache.size=&#34;5M&#34;</pre>
</div>
</div>
<div class="paragraph">
<p>For a more detailed list of recommendations for ZFS-related tuning, see <a href="https://wiki.freebsd.org/ZFSTuningGuide" class="bare">https://wiki.freebsd.org/ZFSTuningGuide</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-links">22.7. Further Resources<a class="anchor" href="#zfs-links"></a></h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://openzfs.org/">OpenZFS</a></p>
</li>
<li>
<p><a href="https://wiki.freebsd.org/ZFSTuningGuide">FreeBSD Wiki - ZFS Tuning</a></p>
</li>
<li>
<p><a href="https://calomel.org/zfs_raid_speed_capacity.html">Calomel Blog - ZFS Raidz Performance, Capacity and Integrity</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="zfs-term">22.8. ZFS Features and Terminology<a class="anchor" href="#zfs-term"></a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>More than a file system, ZFS is fundamentally different. ZFS combines the roles of file system and volume manager, enabling new storage devices to add to a live system and having the new space available on the existing file systems in that pool at once. By combining the traditionally separate roles, ZFS is able to overcome previous limitations that prevented RAID groups being able to grow. A <em>vdev</em> is a top level device in a pool and can be a simple disk or a RAID transformation such as a mirror or RAID-Z array. ZFS file systems (called <em>datasets</em>) each have access to the combined free space of the entire pool. Used blocks from the pool decrease the space available to each file system. This approach avoids the common pitfall with extensive partitioning where free space becomes fragmented across the partitions.</p>
</div>
<table class="tableblock frame-all grid-all stretch informaltable">
<colgroup>
<col style="width: 10%;"/>
<col style="width: 90%;"/>
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-pool"></a>pool</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A storage <em>pool</em> is the most basic building block of ZFS. A pool consists of one or more vdevs, the underlying devices that store the data. A pool is then used to create one or more file systems (datasets) or block devices (volumes).
These datasets and volumes share the pool of remaining free space. Each pool is uniquely identified by a name and a GUID. The ZFS version number on the pool determines the features available.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-vdev"></a>vdev Types</p></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>A pool consists of one or more vdevs, which themselves are a single disk or a group of disks, transformed to a RAID. When using a lot of vdevs, ZFS spreads data across the vdevs to increase performance and maximize usable space. All vdevs must be at least 128 MB in size.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a id="zfs-term-vdev-disk"></a> <em>Disk</em> - The most basic vdev type is a standard block device. This can be an entire disk (such as <span class="filename">/dev/ada0</span> or <span class="filename">/dev/da0</span>) or a partition (<span class="filename">/dev/ada0p3</span>). On FreeBSD, there is no performance penalty for using a partition rather than the entire disk. This differs from recommendations made by the Solaris documentation.</p>
<div class="admonitionblock caution">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-caution" title="Caution"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Using an entire disk as part of a bootable pool is strongly discouraged, as this may render the pool unbootable.
Likewise, you should not use an entire disk as part of a mirror or RAID-Z vdev.
Reliably determining the size of an unpartitioned disk at boot time is impossible and there’s no place to put in boot code.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p><a id="zfs-term-vdev-file"></a> <em>File</em> - Regular files may make up ZFS pools, which is useful for testing and experimentation. Use the full path to the file as the device path in <code>zpool create</code>.</p>
</li>
<li>
<p><a id="zfs-term-vdev-mirror"></a> <em>Mirror</em> - When creating a mirror, specify the <code>mirror</code> keyword followed by the list of member devices for the mirror. A mirror consists of two or more devices, writing all data to all member devices. A mirror vdev will hold as much data as its smallest member. A mirror vdev can withstand the failure of all but one of its members without losing any data.</p>
<div class="admonitionblock note">
<table>
<tbody><tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To upgrade a regular single disk vdev to a mirror vdev at any time, use <code>zpool <a href="#zfs-zpool-attach">attach</a></code>.</p>
</div>
</td>
</tr>
</tbody></table>
</div>
</li>
<li>
<p><a id="zfs-term-vdev-raidz"></a> <em>RAID-Z</em> - ZFS uses RAID-Z, a variation on standard RAID-5 that offers better distribution of parity and eliminates the &#34;RAID-5 write hole&#34; in which the data and parity information become inconsistent after an unexpected restart. ZFS supports three levels of RAID-Z which provide varying levels of redundancy in exchange for decreasing levels of usable storage. ZFS uses RAID-Z1 through RAID-Z3 based on the number of parity devices in the array and the number of disks which can fail before the pool stops  being operational.</p>
<div class="paragraph">
<p>In a RAID-Z1 configuration with four disks, each 1 TB, usable storage is 3 TB and the pool will still be able to operate in degraded mode with one faulted disk. If another disk goes offline before replacing and resilvering the faulted disk would result in losing all pool data.</p>
</div>
<div class="paragraph">
<p>In a RAID-Z3 configuration with eight disks of 1 TB, the volume will provide 5 TB of usable space and still be able to operate with three faulted disks. Sun™ recommends no more than nine disks in a single vdev. If more disks make up the configuration, the recommendation is to divide them into separate vdevs and stripe the pool data across them.</p>
</div>
<div class="paragraph">
<p>A configuration of two RAID-Z2 vdevs consisting of 8 disks each would create something like a RAID-60 array. A RAID-Z group’s storage capacity is about the size of the smallest disk multiplied by the number of non-parity disks. Four 1 TB disks in RAID-Z1 has an effective size of about 3 TB, and an array of eight 1 TB disks in RAID-Z3 will yield 5 TB of usable space.</p>
</div>
</li>
<li>
<p><a id="zfs-term-vdev-spare"></a> <em>Spare</em> - ZFS has a special pseudo-vdev type for keeping track of available hot spares. Note that installed hot spares are not deployed automatically; manually configure them to replace the failed device using <code>zfs replace</code>.</p>
</li>
<li>
<p><a id="zfs-term-vdev-log"></a> <em>Log</em> - ZFS Log Devices, also known as ZFS Intent Log (<a href="#zfs-term-zil">ZIL</a>) move the intent log from the regular pool devices to a dedicated device, typically an SSD. Having a dedicated log device improves the performance of applications with a high volume of synchronous writes like databases. Mirroring of log devices is possible, but RAID-Z is not supported. If using a lot of log devices, writes will be load-balanced across them.</p>
</li>
<li>
<p><a id="zfs-term-vdev-cache"></a> <em>Cache</em> - Adding a cache vdev to a pool will add the storage of the cache to the <a href="#zfs-term-l2arc">L2ARC</a>. Mirroring cache devices is impossible. Since a cache device stores only new copies of existing data, there is no risk of data loss.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-txg"></a> Transaction Group (TXG)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Transaction Groups are the way ZFS groups blocks changes together and writes them to the pool. Transaction groups are the atomic unit that ZFS uses to ensure consistency. ZFS assigns each transaction group a unique 64-bit consecutive identifier. There can be up to three active transaction groups at a time, one in each of these three states:</p>
<p class="tableblock">* <em>Open</em> - A new transaction group begins in the open state and accepts new writes. There is always a transaction group in the open state, but the transaction group may refuse new writes if it has reached a limit. Once the open transaction group has reached a limit, or reaching the <a href="#zfs-advanced-tuning-txg-timeout"><code>vfs.zfs.txg.timeout</code></a>, the transaction group advances to the next state.
* <em>Quiescing</em> - A short state that allows any pending operations to finish without blocking the creation of a new open transaction group. Once all the transactions in the group have completed, the transaction group advances to the final state.
* <em>Syncing</em> - Write all the data in the transaction group to stable storage. This process will in turn change other data, such as metadata and space maps, that ZFS will also write to stable storage. The process of syncing involves several passes. On the first and biggest, all the changed data blocks; next come the metadata, which may take several passes to complete. Since allocating space for the data blocks generates new metadata, the syncing state cannot finish until a pass completes that does not use any new space. The syncing state is also where <em>synctasks</em> complete. Synctasks are administrative operations such as creating or destroying snapshots and datasets that complete the uberblock change. Once the sync state completes the transaction group in the quiescing state advances to the syncing state. All administrative functions, such as <a href="#zfs-term-snapshot"><code>snapshot</code></a> write as part of the transaction group. ZFS adds a created synctask to the open transaction group, and that group advances as fast as possible to the syncing state to reduce the latency of administrative commands.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-arc"></a>Adaptive Replacement Cache (ARC)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ZFS uses an Adaptive Replacement Cache (ARC), rather than a more traditional Least Recently Used (LRU) cache. An LRU cache is a simple list of items in the cache, sorted by how recently object was used, adding new items to the head of the list. When the cache is full, evicting items from the tail of the list makes room for more active objects. An ARC consists of four lists; the Most Recently Used (MRU) and Most Frequently Used (MFU) objects, plus a ghost list for each. These ghost lists track evicted objects to prevent adding them back to the cache. This increases the cache hit ratio by avoiding objects that have a history of occasional use. Another advantage of using both an MRU and MFU is that scanning an entire file system would evict all data from an MRU or LRU cache in favor of this freshly accessed content. With ZFS, there is also an MFU that tracks the most frequently used objects, and the cache of the most commonly accessed blocks remains.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-l2arc"></a>L2ARC</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">L2ARC is the second level of the ZFS caching system. RAM stores the primary ARC. Since the amount of available RAM is often limited, ZFS can also use <a href="#zfs-term-vdev-cache">cache vdevs</a>. Solid State Disks (SSDs) are often used as these cache devices due to their higher speed and lower latency compared to traditional spinning disks. L2ARC is entirely optional, but having one will increase read speeds for cached files on the SSD instead of having to read from the regular disks. L2ARC can also speed up <a href="#zfs-term-deduplication">deduplication</a> because a deduplication table (DDT) that does not fit in RAM but does fit in the L2ARC will be much faster than a DDT that must read from disk. Limits on the data rate added to the cache devices prevents prematurely wearing out SSDs with extra writes. Until the cache is full (the first block evicted to make room), writes to the L2ARC limit to the sum of the write limit and the boost limit, and afterwards limit to the write limit. A pair of <a href="https://man.freebsd.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;format=html">sysctl(8)</a> values control these rate limits. <a href="#zfs-advanced-tuning-l2arc_write_max"><code>vfs.zfs.l2arc_write_max</code></a> controls the number of bytes written to the cache per second, while <a href="#zfs-advanced-tuning-l2arc_write_boost"><code>vfs.zfs.l2arc_write_boost</code></a> adds to this limit during the &#34;Turbo Warmup Phase&#34; (Write Boost).</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-zil"></a>ZIL</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ZIL accelerates synchronous transactions by using storage devices like SSDs that are faster than those used in the main storage pool. When an application requests a synchronous write (a guarantee that the data is stored to disk rather than merely cached for later writes), writing the data to the faster ZIL storage then later flushing it out to the regular disks greatly reduces latency and improves performance. Synchronous workloads like databases will profit from a ZIL alone. Regular asynchronous writes such as copying files will not use the ZIL at all.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-cow"></a>Copy-On-Write</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unlike a traditional file system, ZFS writes a different block rather than overwriting the old data in place. When completing this write the metadata updates to point to the new location. When a shorn write (a system crash or power loss in the middle of writing a file) occurs, the entire original contents of the file are still available and ZFS discards the incomplete write. This also means that ZFS does not require a <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a> after an unexpected shutdown.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-dataset"></a>Dataset</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><em>Dataset</em> is the generic term for a ZFS file system, volume, snapshot or clone. Each dataset has a unique name in the format <em>poolname/path@snapshot</em>. The root of the pool is a dataset as well. Child datasets have hierarchical names like directories. For example, <em>mypool/home</em>, the home dataset, is a child of <em>mypool</em> and inherits properties from it. Expand this further by creating <em>mypool/home/user</em>. This grandchild dataset will inherit properties from the parent and grandparent. Set properties on a child to override the defaults inherited from the parent and grandparent. Administration of datasets and their children can be <a href="#zfs-zfs-allow">delegated</a>.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-filesystem"></a>File system</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A ZFS dataset is most often used as a file system. Like most other file systems, a ZFS file system mounts somewhere in the systems directory hierarchy and contains files and directories of its own with permissions, flags, and other metadata.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-volume"></a>Volume</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ZFS can also create volumes, which appear as disk devices. Volumes have a lot of the same features as datasets, including copy-on-write, snapshots, clones, and checksumming. Volumes can be useful for running other file system formats on top of ZFS, such as UFS virtualization, or exporting iSCSI extents.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-snapshot"></a>Snapshot</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The <a href="#zfs-term-cow">copy-on-write</a> (COW) design of ZFS allows for nearly instantaneous, consistent snapshots with arbitrary names. After taking a snapshot of a dataset, or a recursive snapshot of a parent dataset that will include all child datasets, new data goes to new blocks, but without reclaiming the old blocks as free space. The snapshot contains the original file system version and the live file system contains any changes made since taking the snapshot using no other space. New data written to the live file system uses new blocks to store this data. The snapshot will grow as the blocks are no longer used in the live file system, but in the snapshot alone. Mount these snapshots read-only allows recovering of previous file versions. A <a href="#zfs-zfs-snapshot">rollback</a> of a live file system to a specific snapshot is possible, undoing any changes that took place after taking the snapshot. Each block in the pool has a reference counter which keeps track of the snapshots, clones, datasets, or volumes use that block. As files and snapshots get deleted, the reference count  decreases, reclaiming the free space when no longer referencing a block. Marking snapshots with a <a href="#zfs-zfs-snapshot">hold</a> results in any attempt to destroy it will  returns an <code>EBUSY</code> error. Each snapshot can have holds with a unique name each. The <a href="#zfs-zfs-snapshot">release</a> command removes the hold so the snapshot can deleted. Snapshots, cloning, and rolling back works on volumes, but independently mounting does not.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-clone"></a>Clone</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Cloning a snapshot is also possible. A clone is a writable version of a snapshot, allowing the file system to fork as a new dataset. As with a snapshot, a clone initially consumes no new space. As new data written to a clone uses new blocks, the size of the clone grows. When blocks are overwritten in the cloned file system or volume, the reference count on the previous block decreases. Removing the snapshot upon which a clone bases is impossible because the clone depends on it. The snapshot is the parent, and the clone is the child. Clones can be <em>promoted</em>, reversing this dependency and making the clone the parent and the previous parent the child. This operation requires no new space. Since the amount of space used by the parent and child reverses, it may affect existing quotas and reservations.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-checksum"></a>Checksum</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Every block is also checksummed. The checksum algorithm used is a per-dataset property, see <a href="#zfs-zfs-set"><code>set</code></a>. The checksum of each block is transparently validated when read, allowing ZFS to detect silent corruption. If the data read does not match the expected checksum, ZFS will attempt to recover the data from any available redundancy, like mirrors or RAID-Z. Triggering a validation of all checksums with <a href="#zfs-term-scrub"><code>scrub</code></a>. Checksum algorithms include:</p>
<p class="tableblock">* <code>fletcher2</code>
* <code>fletcher4</code>
* <code>sha256</code>
 The <code>fletcher</code> algorithms are faster, but <code>sha256</code> is a strong cryptographic hash and has a much lower chance of collisions at the  cost of some performance. Deactivating checksums is possible, but  strongly discouraged.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-compression"></a>Compression</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Each dataset has a compression property, which defaults to off. Set this property to an available compression algorithm. This causes compression of all new data written to the dataset. Beyond a reduction in space used, read and write throughput often increases because fewer blocks need reading or writing.</p>
<p class="tableblock"><a id="zfs-term-compression-lz4"></a>
* <em>LZ4</em> - Added in ZFS pool version 5000 (feature flags), LZ4 is now the recommended compression algorithm. LZ4 works about 50% faster than LZJB when operating on compressible data, and is over three times faster when operating on uncompressible data. LZ4 also decompresses about 80% faster than LZJB. On modern CPUs, LZ4 can often compress at over 500 MB/s, and decompress at over 1.5 GB/s (per single CPU core).</p>
<p class="tableblock"><a id="zfs-term-compression-lzjb"></a>
* <em>LZJB</em> - The default compression algorithm. Created by Jeff Bonwick (one of the original creators of ZFS). LZJB offers good compression with less CPU overhead compared to GZIP. In the future, the default compression algorithm will change to LZ4.</p>
<p class="tableblock"><a id="zfs-term-compression-gzip"></a>
* <em>GZIP</em> - A popular stream compression algorithm available in ZFS. One of the main advantages of using GZIP is its configurable level of compression. When setting the <code>compress</code> property, the administrator can choose the level of compression, ranging from <code>gzip1</code>, the lowest level of compression, to <code>gzip9</code>, the highest level of compression. This gives the administrator control over how much CPU time to trade for saved disk space.</p>
<p class="tableblock"><a id="zfs-term-compression-zle"></a>
* <em>ZLE</em> - Zero Length Encoding is a special compression algorithm that compresses continuous runs of zeros alone. This compression algorithm is useful when the dataset contains large blocks of zeros.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-copies"></a>Copies</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">When set to a value greater than 1, the <code>copies</code> property instructs ZFS to maintain copies of each block in the <a href="#zfs-term-filesystem">file system</a> or <a href="#zfs-term-volume">volume</a>. Setting this property on important datasets provides added redundancy from which to recover a block that does not match its checksum. In pools without redundancy, the copies feature is the single form of redundancy. The copies feature can recover from a single bad sector or other forms of minor corruption, but it does not protect the pool from the loss of an entire disk.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-deduplication"></a>Deduplication</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Checksums make it possible to detect duplicate blocks when writing data. With deduplication, the reference count of an existing, identical block increases, saving storage space. ZFS keeps a deduplication table (DDT) in memory to detect duplicate blocks. The table contains a list of unique checksums, the location of those blocks, and a reference count. When writing new data, ZFS calculates checksums and compares them to the list. When finding a match it uses the existing block. Using the SHA256 checksum algorithm with deduplication provides a secure cryptographic hash. Deduplication is tunable. If <code>dedup</code> is <code>on</code>, then a matching checksum means that the data is identical. Setting <code>dedup</code> to <code>verify</code>, ZFS performs a byte-for-byte check on the data ensuring they are actually identical. If the data is not identical, ZFS will note the hash collision and store the two blocks separately. As the DDT must store the hash of each unique block, it consumes a large amount of memory. A general rule of thumb is 5-6 GB of ram per 1 TB of deduplicated data). In situations not practical to have enough RAM to keep the entire DDT in memory, performance will suffer greatly as the DDT must read from disk before writing each new block. Deduplication can use L2ARC to store the DDT, providing a middle ground between fast system memory and slower disks. Consider using compression instead, which often provides nearly as much space savings without the increased memory.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-scrub"></a>Scrub</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Instead of a consistency check like <a href="https://man.freebsd.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;format=html">fsck(8)</a>, ZFS has <code>scrub</code>. <code>scrub</code> reads all data blocks stored on the pool and verifies their checksums against the known good checksums stored in the metadata. A periodic check of all the data stored on the pool ensures the recovery of any corrupted blocks before needing them. A scrub is not required after an unclean shutdown, but good practice is at least once every three months. ZFS verifies the checksum of each block during normal use, but a scrub makes certain to check even infrequently used blocks for silent corruption. ZFS improves data security in archival storage situations. Adjust the relative priority of <code>scrub</code> with <a href="#zfs-advanced-tuning-scrub_delay"><code>vfs.zfs.scrub_delay</code></a> to prevent the scrub from degrading the performance of other workloads on the pool.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-quota"></a>Dataset Quota</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ZFS provides fast and accurate dataset, user, and group space accounting as well as quotas and space reservations. This gives the administrator fine grained control over space allocation and allows reserving space for critical file systems.</p>
<p class="tableblock">ZFS supports different types of quotas: the dataset quota, the <a href="#zfs-term-refquota">reference quota (refquota)</a>, the <a href="#zfs-term-userquota">user quota</a>, and the <a href="#zfs-term-groupquota">group quota</a>.</p>
<p class="tableblock">Quotas limit the total size of a dataset and its descendants, including snapshots of the dataset, child datasets, and the snapshots of those datasets.</p>
<p class="tableblock">[NOTE]
====
Volumes do not support quotas, as the <code>volsize</code> property acts as an implicit quota.
====</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-refquota"></a>Reference Quota</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A reference quota limits the amount of space a dataset can consume by enforcing a hard limit. This hard limit includes space referenced by the dataset alone and does not include space used by descendants, such as file systems or snapshots.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-userquota"></a>User Quota</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">User quotas are useful to limit the amount of space used by the specified user.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-groupquota"></a>Group Quota</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The group quota limits the amount of space that a specified group can consume.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-reservation"></a>Dataset Reservation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The <code>reservation</code> property makes it possible to guarantee an amount of space for a specific dataset and its descendants. This means that setting a 10 GB reservation on <span class="filename">storage/home/bob</span> prevents other datasets from using up all free space, reserving at least 10 GB of space for this dataset. Unlike a regular <a href="#zfs-term-refreservation"><code>refreservation</code></a>, space used by snapshots and descendants is not counted against the reservation. For example, if taking a snapshot of <span class="filename">storage/home/bob</span>, enough disk space other than the <code>refreservation</code> amount must exist for the operation to succeed. Descendants of the main data set are not counted in the <code>refreservation</code> amount and so do not encroach on the space set.</p>
<p class="tableblock">Reservations of any sort are useful in situations such as planning and testing the suitability of disk space allocation in a new system, or ensuring that enough space is available on file systems for audio logs or system recovery procedures and files.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-refreservation"></a>Reference Reservation</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The <code>refreservation</code> property makes it possible to guarantee an amount of space for the use of a specific dataset <em>excluding</em> its descendants. This means that setting a 10 GB reservation on <span class="filename">storage/home/bob</span>, and another dataset tries to use the free space, reserving at least 10 GB of space  for this dataset. In contrast to a regular <a href="#zfs-term-reservation">reservation</a>, space used by snapshots and descendant datasets is not counted against the reservation. For example, if taking a snapshot of <span class="filename">storage/home/bob</span>, enough disk space other than the <code>refreservation</code> amount must exist for the operation to succeed. Descendants of the  main data set are not counted in the <code>refreservation</code> amount and so do not encroach on the space set.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-resilver"></a>Resilver</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">When replacing a failed disk, ZFS must fill the new disk with the lost data. <em>Resilvering</em> is the process of using the parity information distributed across the remaining drives to calculate and write the missing data to the new drive.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-online"></a>Online</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A pool or vdev in the <code>Online</code> state has its member devices connected and fully operational. Individual devices in the <code>Online</code> state are functioning.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-offline"></a>Offline</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The administrator puts individual devices in an <code>Offline</code> state if enough redundancy exists to avoid putting the pool or vdev into a <a href="#zfs-term-faulted">Faulted</a> state. An administrator may choose to offline a disk in preparation for replacing it, or to make it easier to identify.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-degraded"></a>Degraded</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A pool or vdev in the <code>Degraded</code> state has one or more disks that disappeared or failed. The pool is still usable, but if other devices fail, the pool may become unrecoverable. Reconnecting the missing devices or replacing the failed disks will return the pool to an <a href="#zfs-term-online">Online</a> state after the reconnected or new device has completed the <a href="#zfs-term-resilver">Resilver</a> process.</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><a id="zfs-term-faulted"></a>Faulted</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A pool or vdev in the <code>Faulted</code> state is no longer operational. Accessing the data is no longer possible. A pool or vdev enters the <code>Faulted</code> state when the number of missing or failed devices exceeds the level of redundancy in the vdev. If reconnecting missing devices the pool will return to an <a href="#zfs-term-online">Online</a> state. Insufficient redundancy to compensate for the number of failed disks loses the pool contents and requires restoring from backups.</p></td>
</tr>
</tbody>
</table>
</div>
</div>

    </div>
    
    <hr />
    <div class="last-modified">
      <p><strong>Last modified on</strong>: December 22, 2023 by <a href="https://cgit.freebsd.org/doc/commit/?id=7852314" target="_blank">fiercex</a></p>
    </div>
    
    <div class="buttons">
      
      <div class="prev">
        <i class="fa fa-angle-left" aria-hidden="true" title="Prev"></i>
        <div class="container">
          
            <a href=https://free.bsd-doc.org/zh-cn/books/handbook/geom class="direction">Prev</a>
          
        </div>
      </div>
      
      <div class="home">
        <i class="fa fa-home" aria-hidden="true" title="Home"></i>
        <div class="container">
          
            <a href="../" class="direction">Home</a>
          
        </div>
      </div>
      
      <div class="next">
        <div class="container">
          
            <a href=https://free.bsd-doc.org/zh-cn/books/handbook/filesystems  class="direction">Next</a>
          
        </div>
        <i class="fa fa-angle-right" aria-hidden="true" title="Next"></i>
      </div>
      
    </div>
    <label class="hidden book-menu-overlay" for="menu-control"></label>
  </div>
  <aside class="toc">
    <div class="toc-content">
      <h3>Table of Contents</h3>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#zfs-differences">22.1. What Makes ZFS Different</a></li>
    <li><a href="#zfs-quickstart">22.2. Quick Start Guide</a></li>
    <li><a href="#zfs-zpool">22.3. <code>zpool</code> Administration</a></li>
    <li><a href="#zfs-zfs">22.4. <code>zfs</code> Administration</a></li>
    <li><a href="#zfs-zfs-allow">22.5. Delegated Administration</a></li>
    <li><a href="#zfs-advanced">22.6. Advanced Topics</a></li>
    <li><a href="#zfs-links">22.7. Further Resources</a></li>
    <li><a href="#zfs-term">22.8. ZFS Features and Terminology</a></li>
  </ul>
</nav>
      <hr />
      
    </div>
  </aside>
  <a class="to-top" href="#top">
    <i class="fa fa-arrow-circle-up" aria-hidden="true"></i>
  </a>
</main>

    <footer>
  <div class="footer-container">
    <section class="logo-column">
          <img src="https://free.bsd-doc.org/images/FreeBSD-colors.svg" width="160" height="50" alt="FreeBSD logo" />
        <div class="options-container">
          
            <div class="language-container">
              <a id="languages" href="https://free.bsd-doc.org/zh-cn/languages">
                
                <img src="https://free.bsd-doc.org/images/language.png" class="language-image" alt="Choose language">
                <span>简体中文</span>
              </a>
            </div>
          
          <div class="theme-container">
            <select id="theme-chooser">
	      <option value="theme-system">System</option>
              <option value="theme-light">Light</option>
              <option value="theme-dark">Dark</option>
              <option value="theme-high-contrast">High contrast</option>
            </select>
          </div>
        </div>
      </section>
      
      <section class="copyright-column">
        <p>&copy; 1994-2023 The FreeBSD Project. All rights reserved</p>
        <span>Made with <span class="heart">♥</span> by the FreeBSD Community</span>
      </section>
  </div>
</footer>

  </body>
</html>
